PROBLEM TO BE SOLVED: To maintain the equivalent data reading speed as a cash memory in the form of the parallel operation of an address array and a data array, and to realize low power consumption. SOLUTION: A CPU 101 outputs a fist signal 104 indicating whether or not this time access is a continuous access address to the previous access address. A cache memory 4 is provided with a flag means which is turned into a first state when a cache line to be selected by an index operation in the next access should be the same as that in this time access when it is indicated that the first signal is the continuous access address in the next access by the CPU, and which is turned into a second state otherwise. When it is indicated that the first signal is the continuous access address at the time of access by the CPU, and the flag means is turned into the first state, the memory operation of an address array 109 is suppressed, and only a data array 110 is operated..(57) [Summary] [PROBLEMS] To realize low power consumption while maintaining a data reading speed equivalent to that of a cache memory in which an address array and a data array are operated in parallel. A CPU (101) outputs a first signal (104) indicating whether a current access is a continuous access address with respect to a previous access address. The cache memory (4) becomes the first in the next access by the CPU.  If the signal indicates a continuous access address, the cache line selected by the index operation in the next access should be in the first state if it should be the same as the current access, and if not, it should be in the second state. It has a flag means (313).  When the signal indicates a continuous access address and the flag means is in the first state, the address array (10  The memory operation of 9) is suppressed, and only the data array (110) is operated. .1. An access control means for outputting an address signal, and a cache memory means connected to the access control means, wherein the access control means performs a continuous access to a previous access address. Outputting a first signal indicating whether the address is an address, wherein the cache memory means has a first memory unit having a plurality of tag storage units and a plurality of cache lines corresponding to the individual tag storage units. 2 memory unit, a comparison unit that determines whether a tag of a tag storage unit selected by the first part of the access address signal matches the second part of the access address signal, and a comparison between the comparison unit And a control unit for inputting a result, wherein the control unit determines that the first signal is the continuous access in the next access by the access control unit. A flag means for setting the first state if the determination result of the comparison unit in the next access should be the same in the same cache line as the current access if it indicates a dress, and setting the second state otherwise. Wherein the first signal at the time of access by the access control means indicates a continuous access address, and the flag means  A data processing device that, when in a state, inhibits a memory operation of the first memory unit and selects a cache line of the second memory unit by a first part of an access address signal..2. An access control means for outputting an address signal, and a cache memory means connected to the access control means, wherein the access control means performs a continuous access to a previous access address. Outputting a first signal indicating whether the address is an address, wherein the cache memory means has a first memory unit having a plurality of tag storage units and a plurality of cache lines corresponding to the individual tag storage units. 2 memory unit, a comparison unit that determines whether a tag of a tag storage unit selected by the first part of the access address signal matches the second part of the access address signal, and a comparison between the comparison unit A control unit for inputting a result, wherein the control unit includes a flag unit, and the first signal is used in a next access by the access control unit. If it indicates that the address is a continuous access address, it is determined whether the determination result of the comparison unit in the next access is the same in the same cache line as the current access, and if it is the same, the flag means is set to the first state. If not, a flag control means for setting the flag means to the second state;  When the access control means accesses, the first signal indicates a continuous access address, and when the flag means is in the first state, the memory operation of the first memory section is suppressed and the second memory section is disabled. A cache line is selected by the first portion of the access address signal, otherwise, the operations of the first memory unit and the second memory unit are permitted, and the cache of the tag storage unit and the second memory unit of the first memory unit are allowed. A memory operation control means for selecting a line by a first portion of the access address signal..3. The flag control means inputs a third part of an access address signal output by the access control means, which is allocated to selection of an access unit area of a cache line, and a comparison result of the comparison unit. When the comparison result is a cache hit state and the third part of the access address signal indicates an area other than the last access unit area of the cache line, the flag means is set to the first state; 3. The data processing apparatus according to claim 2, wherein said flag means is set to a second state when a third portion of the access address signal indicates the last access unit area of said cache line..4. The flag control means further performs a cache fill operation in response to the result of the comparison being a cache miss state, and outputs a third portion of the access address signal relating to the cache miss state to a cache line. 4. The data processing apparatus according to claim 3, wherein the flag unit is set to the first state when indicating an area other than the last access unit area..5. The cache memory means, wherein the first memory unit and the second memory unit are divided into a plurality of ways, the comparison unit performs the match determination for each way, and based on the match determination result for each way. A hit way selector for selecting the read data of the second memory unit in a way unit, wherein the memory operation control unit is configured to synchronize each of the ways by the comparing unit in synchronization with a cache hit state. Register means for loading a determination result; and when the flag means is in a first state and the first signal indicates a continuous access address, an output of the register means is supplied to the hitway selector. When the flag means is in a second state or indicates that the first signal is not a continuous access address, And way selection information selector for supplying an output to the hit way selector, further comprising Claim 3 data processing apparatus according to..6. The cache memory means, wherein the first memory unit and the second memory unit are divided into a plurality of ways, the comparison unit performs the match determination for each way, and based on the match determination result for each way. A hit way selector for selecting read data of the second memory unit in units of a way, wherein the memory operation control means responds to a cache hit state for each way by the comparing unit. Register means for loading the determination result and loading the designation information of the way subjected to the cache fill operation in response to the cache miss state; and outputting the output of the register means based on the first state of the flag means. The output of the comparing section is supplied to the hitway selector, and the output of the comparing section is output based on the second state of the flag means. The data processing apparatus further comprising fourth aspect and way selection information selector supplies the way selector, a..7. The data processing device according to claim 1, wherein said second memory unit is for storing an instruction fetched by said access control unit..8. The method according to claim 1, wherein the first signal is a non-operation,  A signal indicating a normal instruction fetch related to the instruction fetch and a forced instruction fetch related to the instruction fetch by a unique code. The code assigned to the normal instruction fetch indicates that the current access is a continuous access address with respect to the previous access address. 8. The data processing apparatus according to claim 7, wherein the code assigned to the forced instruction fetch means that the current access is a discontinuous access address with respect to the previous access address..9. The second memory unit is used for storing an instruction fetched by the access control unit and data to be accessed by the access control unit, and the first signal is used to store a current instruction access address with respect to a previous instruction access address. A signal indicating whether or not the instruction access is a continuous access address; and the flag control means is a third one assigned to the selection of the access unit area of the cache line among the access address signals output by the access control means. A part and a comparison result of the comparison unit are input, and in the instruction access, the comparison result by the comparison unit is a cache hit state, and the third part of the access address signal is an area other than the last access unit area of the cache line. The flag means is set to the first state, and the comparison result indicates a cache miss state or the Claim third portion of the access address signal is one which in the second state the flag means when referring to the last access unit area of the cache line 2  The data processing device according to claim 1..10. The cache memory means, wherein the first memory unit and the second memory unit are divided into a plurality of ways, the comparison unit performs the match determination for each way, and based on the match determination result for each way. A hit way selector for selecting the read data of the second memory unit in units of a way, wherein the set memory is in a set associative form. The first signal is a signal indicating whether or not a current instruction access is a continuous access address with respect to a previous instruction access address. In response to the cache hit state in the access, the determination result for each way by the comparing unit is loaded and the instruction is issued. A way register for loading the designation information of the way targeted for the cache fill operation in response to a cache miss state in the instruction access, and an output of the way register based on the first state of the flag means. A way selection information selector for supplying the output of the comparison unit to the hit way selector based on a second state of the flag means, and a first address of an access address signal related to a cache hit at the time of instruction access. Address register means for loading a first part of an access address signal relating to a cache miss at the time of instruction access. The flag control means comprises: a cache line of an access address signal output by the access control means; Third assigned to selection of access unit area And the comparison result of the comparison unit are input, and in the instruction access, the comparison result by the comparison unit is a cache hit state, and the third part of the access address signal is an area other than the last access unit area of the cache line. Or the cache fill operation is performed in response to the comparison result indicating a cache miss state in the instruction access, and the third part of the access address signal related to the cache miss state is the last access unit of the cache line. The flag means is set to the first state when indicating an area other than the area, and when the comparison result is a cache miss state in the instruction access, the third part of the access address signal is the last access unit of the cache line in the instruction access. When indicating an area or data access 3. The data processing apparatus according to claim 2, wherein the second state is set when a way and a cache line to be cache-filled by the cache miss state are specified by the values held in the way register means and the address register means. ..11. The first signal is a signal indicating a non-operation, a data access, a normal instruction fetch related to an instruction fetch, and a forced instruction fetch related to an instruction fetch by respective unique codes, and is assigned to the normal instruction fetch. The code means that the current access is a continuous access address with respect to the previous access address, and the code assigned to forced instruction fetch is that the current access is a non-contiguous access address with respect to the previous access address. The data processing device according to claim 9 or 10, which means that there is..12. An access control means for outputting an address signal, and a cache memory means connected to said access control means, wherein said access control means makes a current access to a previous access address a continuous access. Outputting a first signal indicating whether the address is an address, wherein the cache memory means has a first memory unit having a plurality of tag storage units and a plurality of cache lines corresponding to the individual tag storage units. A second memory unit; a buffer unit capable of storing storage information of the cache line selected in the second memory unit;  A comparison unit that determines whether a tag of the tag storage unit selected by the part matches the second part of the access address signal, and a control unit that inputs a comparison result of the comparison unit, The control unit, if the first signal indicates the continuous access address in the next access by the flag unit and the access control unit, the determination result of the comparison unit in the next access is the same cache line as the current access. A flag control unit that sets the flag unit to the first state if it is to be matched, and sets the flag unit to the second state if not, and an access by the access control unit. Sometimes, in response to the flag unit being set to the first state, the storage information of the cache line selected in the second memory unit is stored in the buffer unit. Transfer, indicating that the first signal is a continuous access address at the time of access by the access control means, and inhibiting the first memory unit and the second memory operation when the flag means is in the first state. The data stored in the buffer unit is selected, and if not, the operation of the first memory unit and the second memory unit is allowed, and the tag storage unit of the first memory unit and the cache line of the second memory unit are accessed. A memory operation control means for selecting the first part of the signal..13. The flag control means inputs a third part of an access address signal output by the access control means, which is allocated to selection of an access unit area of a cache line, and a comparison result of the comparison unit. When the comparison result is a cache hit state and the third part of the access address signal indicates an area other than the last access unit area of the cache line, the flag means is set to the first state; 13. The data processing apparatus according to claim 12, wherein said flag means is set to a second state when a third part of the access address signal indicates the last access unit area of said cache line..14. The flag control unit further performs a cache fill operation in response to the result of the comparison being a cache miss state, and sets a third portion of the access address signal related to the cache miss state to a cache line. 14. The data processing apparatus according to claim 13, wherein the flag means is set to the first state when indicating an area other than the last access unit area..15. The data processing apparatus according to claim 1, wherein said access control means is a central processing unit provided on a same semiconductor substrate together with a cache memory means..16. The data processing apparatus according to claim 1, wherein said access control means is a digital signal processing means provided on the same semiconductor substrate together with a cache memory means..17. A data processing system, comprising: the data processing device according to claim 15; memory means accessed by the data processing device; and a peripheral circuit controlled by the data processing device..18. The second memory section is for storing data to be accessed by the access control means, and the first signal is a signal indicating whether a current data access is a continuous access address with respect to a previous data access address. A flag indicating whether or not a third portion of the access address signal output by the access control unit, which is assigned to the selection of the access unit area of the cache line, is compared with the comparison result of the comparison unit. Inputting the flag means to a first state when the result of comparison by the comparing unit in the data access is a cache hit state and the third part of the access address signal indicates an area other than the last access unit area of the cache line; If the result of the comparison is a cache miss condition or the third part of the access address signal is Serial data processor according to claim 2, wherein said flag means when indicating the last access unit area of the cache line is to the second state..19. A memory access control circuit including an address supply circuit for outputting an address signal, a plurality of memories each storing data, and a control circuit controlling access to the plurality of memories, wherein the control circuit Responding to a signal indicating that the current address signal is continuous with respect to the previous address signal output from the memory access control circuit, and selectively accessing the memory selected by the previous address signal. A data processing device characterized in that:.DETAILED DESCRIPTION OF THE INVENTION[0001]BACKGROUND OF THE INVENTION 1. Field of the Invention The present invention relates to a data processing apparatus having a cache memory, and more particularly to a technique for realizing reduction of power consumption of a cache memory and early determination of read data, for example, a microprocessor having a cache memory for instructions. It is related to technology that is effective when applied to The present invention also relates to a technique applicable to a data processing device having a system memory of a preceding operation type without being limited to a cache memory.[0002]2. Description of the Related Art A cache memory temporarily stores recently used information among information stored in a main storage means or the like, and reads or writes stored information at high speed by an associative operation. The set associative cache memory has a plurality of ways, and each way includes an address section having a tag storage area and the like, and a data section having a cache line. One tag storage area corresponds to one cache line for each way, and in the associative operation of the cache memory, the tag storage area and the cache line for each way are selected by an index operation using a part of the address signal. A cache hit / miss determination is made based on whether or not the selected tag matches a predetermined part of the access address signal (address comparison). In the case of a cache hit, the information of the corresponding cache line is output, or information is written to the cache line.If the cache memory does not have a mechanism for predicting a cache hit way, it is necessary to check the correspondence between tags and access address signals by indexing all address portions of the ways. Since a cache hit only occurs in one way, the operation of the remaining ways will be meaningless, and the power consumption in that part will also be wasted.[0004] Regarding the operation of the data array, the case where only the data array of the hit way is operated after the cache hit / miss determination is determined, and the operation of the data array of all the ways is started before the determination is made. There are times when you do.In the former case, the operation of the address array in which a cache miss has occurred is wasted, but since only the necessary way is operated for the data array, there is little waste of power consumption. However, in that case, the data access related to the cache hit becomes slow, and the memory access by the CPU or the like cannot be speeded up.In the latter case, the operation of the data array is started without waiting for the operation time of the address array, so that the memory access speed can be increased. But,  The power consumption of the way in which a cache miss occurred is substantially wasted. That is, by indexing the data array and the address array in parallel, the data array is operated in parallel with the address array. Data read from each way of the data array is held in a data latch. The output of the data latch is performed by a selector based on the address comparison for each address array, and the selector selects the data latch of the way whose address comparison matches. Since the address array and the data array are operated in parallel, the time lag of data reading can be reduced, but the operation of the address array and the data array in which a cache miss has occurred is wasted, and the power consumption increases accordingly.Japanese Patent Application Laid-Open No. Hei 7-3 discloses a technique for reducing power consumption and reducing a time lag in reading data in a set associative cache memory.  No. 34423 is known. The technology described in this document is for set-associative cache memory.  A way prediction unit that determines a predicted value of a way determination signal indicating whether a way has been hit; a unit that activates only a data storage unit of a way predicted based on a predicted value from the way prediction unit; A way determination signal from a way determiner to be output is compared with the predicted value. A way determination that indicates a way that hits the predicted value of the way prediction unit when the determination result by the way prediction determination unit does not match. Means for changing to a signal. According to this, since only the way that has been predicted hit is operated, low power consumption of the cache memory and a time lag of data reading can be reduced.[0008]In the above technique, the way determiner determines whether the cache tag matches a part of the access address, and the result of the determination is a way determination signal for each way. The way prediction unit holds a way determination signal in a latch. The value held in the latch is compared with the way determination signal. If the values match, the cache line selection operation is performed on the data array of the predicted way. Thus, high-speed access can be realized as in the case where the data array is operated in parallel with the operation of the address array. At this time, at least the predicted way address array must be operated. If the operation of all address arrays is stopped, the validity of the predicted way cannot be determined. In this regard, it has been clarified by the present inventors that there is room for further lower power consumption.If the value of the latch does not match the way determination signal, the normal way must be operated in accordance with the predicted value of the way predictor. Therefore, the preceding operation on the data array of the predicted way becomes meaningless, and the index operation on the regular data array must be performed again. Therefore, the present inventor has found that high-speed access may be hindered as compared with the case where the data array is operated in parallel with the operation of the address array.An object of the present invention is to provide a data processing device capable of realizing both low power consumption of a cache memory and high-speed reading of data.Another object of the present invention is to provide a data processing device capable of realizing both low power consumption and high-speed access to a memory.The above and other objects and novel features of the present invention will become apparent from the description of the present specification and the accompanying drawings.[0013]The following is a brief description of an outline of a typical invention among the inventions disclosed in the present application.(1) The data processing device includes access control means for outputting an address signal, and cache memory means connected to the access control means. The access control means outputs a first signal indicating whether a current access is a continuous access address with respect to a previous access address. The cache memory means includes a first memory unit having a plurality of tag storage units, a second memory unit having a plurality of cache lines corresponding to the individual tag storage units, and a first portion (index) of an access address signal. A comparison unit that performs an address comparison to determine whether a tag (cache tag) of a tag storage unit selected by the address) matches a second part (tag address) of the access address signal, and a comparison result of the comparison unit And a control unit for inputting The control unit, if the first signal indicates the continuous access address in the next access by the access control unit, the determination result of the comparison unit in the next access is the same in the same cache line as the current access. Flag means for setting the state to the first state when it should be, and setting the state to the second state otherwise;  1 indicates that the signal is a continuous access address and when the flag means is in the first state, the memory operation of the first memory unit is inhibited and the cache line of the second memory unit is set to the first address of the access address signal. Select in the part.In one cache line, data of the same tag (data having a common tag) is stored in the order of addresses. For example, if the data size of one cache line is 16 bytes, when selecting one byte of data from the data of one cache line, the lower 4 bits of an access address signal as a byte address may be used. The same cache line is selected for a continuous access address in which the upper side of the access address signal is the same and the lower 4 bits are continuously changed. The first signal is information indicating whether a current access address signal is continuous with respect to a previous access address signal. At the time of memory access by the continuous access address signal, if it is clear that the cache lines that become cache hits are the same, the address comparison operation using the first memory unit (address array) becomes unnecessary, and the address becomes unnecessary. No array operation is required. The flag means assures, by the first state, that the cache line that becomes a cache hit is the same in the next access by the continuous access address. When the first signal is in the continuous access address state and the flag means is in the first state, a cache hit occurs on the same cache line as the previous access. Therefore, in this case, even if the operation of the address array is suppressed, the cache line selected (indexed) in the second memory unit (data array) becomes a cache line including a cache entry valid for the access address signal. . The control unit realizes low power consumption by suppressing the operation of the address array when detecting the state. Further, as is clear from the above, since the control is not based on a simple expectation such as simply remembering the index address of the previous access (the first part of the access address signal) and comparing this with the current index address, An error does not occur in the determination result, and there is no case where the operation is restarted from the address determination again, and the delay of the data read operation can be completely prevented. Low power consumption can be realized while maintaining a data reading speed equivalent to that of a format in which an address array and a data array are operated in parallel.(2) In the above (1), the control unit comprises:  In the next access by the flag means and the access control means, if the first signal indicates the continuous access address, is the determination result of the comparing section in the next access coincident with the same cache line as the current access? And a flag control means for setting the flag means to the first state if they should match, and a flag control means for setting the flag means to the second state otherwise. When the flag means indicates a continuous access address and the flag means is in the first state, the memory operation of the first memory unit is inhibited, and the cache line of the second memory unit is set to the first part of the access address signal. If not, allow the operation of the first memory unit and the second memory unit and allow the tag storage unit of the first memory unit It can be constituted by the second memory operation control unit for selecting a memory of the cache line in the first part of the access address signal.(3) The flag control means can be configured as follows. That is, the flag control means may include third address information such as offset address information assigned to selection of an access unit area of a cache line in the access address signal output by the access control means.  And the comparison result of the comparison unit, and when the comparison result is in a cache hit state and the third part of the access address signal indicates an area other than the last access unit area of the cache line, the flag means is turned on. When the comparison result indicates the cache miss state or the third portion of the access address signal indicates the last access unit area of the cache line, the flag means is set to the second state.  State. Accordingly, in the next access, if the flag means is set to the first state, the cache line selected in the next access will not be accessible on the condition that the first signal indicates a continuous access address. It will be understood that this is the same as the cache line selected in.(4) In the state control of the flag means, a cache fill can be considered. That is, when the cache fill is not considered in the state control of the flag unit, the flag unit is not set to the first state even if the cache fill is performed due to a cache miss. Therefore,  After the cache fill, even if the cache line related to the cache fill is selected again, since the flag means is in the second state, the address determination operation is performed by operating all the address arrays. The low power consumption state cannot be realized. Therefore, after the cache fill, the flag control means further sets the comparison result in the cache miss state so that the operation of the address array can be suppressed even when the cache line related to the cache fill is selected again. Setting the flag means to the first state when a cache fill operation is performed in response to the fact and the third part of the access address signal relating to the cache miss state indicates an area other than the last access unit area of the cache line; Can be. This example is shown in FIG. 10 and FIG.(5) The cache memory means may be set associative or direct map. In the case of the set associative format, the cache memory means includes a first memory unit and a second memory unit that are divided into a plurality of ways, the comparison unit performs the match determination for each way, and the match determination result for each way. And a hit way selector for selecting the read data of the second memory unit on a way basis based on the data. in this case,  When the flag means is set to the first state and the first signal indicates a continuous access address, the way to be hit can be specified without depending on the operation of the address array and the address comparison operation. Wherein the memory operation control means includes a register means for loading a determination result for each way by the comparing section in synchronization with a cache hit state, the flag means being in a first state, and the first signal being continuously output. The output of the register means is supplied to the hitway selector when indicating that the address is an access address, and when the flag means is set to the second state or when the first signal indicates that the address is not a continuous access address, the comparing section is output. And a way selection information selector that supplies the output of the hit way selector to the hit way selector.(6) In (4), in which the flag means is controlled in consideration of the cache fill as well, similarly to (5), the way to be hit can be specified without incurring the address array operation and the address comparison operation. In this case, the register means may store the selected way at the time of the cache fill operation to be set to the first state. That is, the memory operation control unit loads the determination result for each way by the comparing unit in response to the cache hit state and specifies the way specified as the target of the cache fill operation in response to the cache miss state. The register means to be loaded, and the output of the register means is supplied to the hitway selector based on the first state of the flag means, and the output of the comparing section is supplied to the hitway selector based on the second state of the flag means. And a way selection information selector to be supplied to the selector.(7) The cache memory means may be an instruction cache memory. That is, the second  The memory unit is used for storing an instruction fetched by the access control unit.(8) When the cache memory means is used as an instruction cache memory, the first signal includes a non-operation (NOP), a normal instruction fetch (NIF) for an instruction fetch, and a forced instruction fetch (FIF) for an instruction fetch. ) May be any signal containing information indicating each of them by a unique code. At this time, the code assigned to the normal instruction fetch means that the current access is a continuous access address to the previous access address, and the code assigned to the forced instruction fetch is Means that the current access is a discontinuous access address.(9) The cache memory means can be used as a unified cache memory.  That is, in the above (2), the second memory unit is used for storing an instruction fetched by the access control unit and data accessed by the access control unit. The first signal is a signal indicating whether or not the current instruction access is a continuous access address with respect to the previous instruction access address. At this time, if a cache fill is performed due to a data access cache miss, the cache entry of the instruction may be evicted from the cache line. If the evicted cache line includes a part of the instruction currently used for the instruction fetch, there is a risk of malfunctioning as it is. To cope with this, the flag means is unconditionally set to the second in the event of a cache miss.  What is necessary is just to be in a state. That is, the flag control means:  A third part of the access address signal output by the access control means, which is allocated to the selection of an access unit area of a cache line, and a comparison result of the comparison unit are input, and the comparison result by the comparison unit in an instruction access is stored in a cache. In a hit state and when the third portion of the access address signal indicates an area other than the last access unit area of the cache line, the flag means is set to the first state, and the comparison result is a cache miss state or the third state of the access address signal. When the part 3 indicates the last access unit area of the cache line, the flag means is set to the second access unit area.  The state should just be.(10) When the set associative type cache memory means is a unified cache memory similar to the above, the flag means can be set to the first state in consideration of a cache full at the time of instruction access. In the cache fill at the time of data access, eviction of an instruction cache entry can be considered. That is, in the above (2), in the cache memory means, the first memory unit and the second memory unit are divided into a plurality of ways, the comparison unit performs the match determination for each way, and the match determination for each way. It is set associative type provided with a hit way selector for selecting the read data of the second memory unit on a way basis based on the result. The second memory unit is used for storing an instruction fetched by the access control unit and data to be accessed by the access control unit. The first signal is a signal indicating whether or not the current instruction access is a continuous access address with respect to the previous instruction access address. The memory operation control unit loads a determination result for each way by the comparing unit in response to a cache hit state in an instruction access and a cache fill operation target in response to a cache miss state in an instruction access. And an output of the way register means is supplied to the hit way selector based on a first state of the flag means, and the comparing section is supplied based on a second state of the flag means. And a first part of an access address signal relating to a cache hit at the time of instruction access or a first part of an access address signal relating to a cache miss at the time of instruction access are supplied to the hit way selector. Address register means to be loaded. The flag control means inputs a third part of the access address signal output by the access control means, which is assigned to selection of an access unit area of a cache line, and a comparison result of the comparison unit, and performs the comparison in instruction access. The result of the comparison is a cache hit state and the third  Indicates a region other than the last access unit region of the cache line, or the cache-fill operation is performed in response to the comparison result indicating a cache miss state in the instruction access, and the access related to the cache miss state is performed. When the third portion of the address signal indicates an area other than the last access unit area of the cache line, the flag means is set to the first state, and when the comparison result indicates a cache miss state in the instruction access, the access address signal is determined in the instruction access. When the third part indicates the last access unit area of the cache line, or the way and the cache line to be cache-filled by the cache miss state in the data access are determined by the values held in the way register means and the address register means. The second state when it is intended to be constant.Thus, the flag means can be set to the first state by the cache fill at the time of instruction access. Further, when the instruction cache line of interest is evicted due to a cache fill at the time of a cache miss in data access, the flag means can be reset to the second state through the operation of comparing the access way and the index address.(11) In the case of the unified cache, the first signal is a signal containing information indicating non-operation, data access, normal instruction fetch related to instruction fetch, and forced instruction fetch related to instruction fetch by respective unique codes. It can be. The code assigned to the normal instruction fetch means that the current access is a continuous access address with respect to the previous access address, and the code assigned to the forced instruction fetch is the current access address with respect to the previous access address. It means that the access is a non-contiguous access address.(12) In order to further reduce the power consumption due to the operation of the data array, a buffer unit of the cache line can be employed. That is, the data processing device includes access control means for outputting an address signal, and cache memory means connected to the access control means. The access control means outputs a first signal indicating whether a current access is a continuous access address with respect to a previous access address. The cache memory means includes a first memory unit having a plurality of tag storage units, a second memory unit having a plurality of cache lines corresponding to the individual tag storage units, and a cache selected in the second memory unit. A buffer unit that can store the storage information of the line, a comparing unit that determines whether a tag of the tag storage unit selected by the first part of the access address matches the second part of the access address, A control unit for inputting a comparison result of the comparison unit. The control unit has a flag unit, and in the next access by the access control unit, if the first signal indicates the continuous access address, the determination result of the comparison unit in the next access is the same as the current access. It has a flag control means for judging whether or not a match is made in the cache line, and setting the flag means to the first state if it is to be matched, and setting the flag means to the second state otherwise.  Further, the control unit internally transfers the storage information of the cache line selected in the second memory unit to the buffer unit in response to the flag unit being set to the first state at the time of access by the access control unit. The first signal indicates that the address is a continuous access address at the time of access by the access control means, and the flag means indicates the first access signal.  When in the state, the operation of the first memory unit and the second memory unit is suppressed and the data stored in the buffer unit is selected. Otherwise, the operation of the first memory unit and the second memory unit is permitted. A memory operation control means for selecting a tag storage unit of one memory unit and a cache line of the second memory unit based on the first part of the access address signal.According to this, in the continuous access to the data of the cache line once in the cache hit state, the operation of all the memory units can be stopped, and the power consumption can be further reduced. In addition, the data for one cache line required at that time is saved in the buffer unit of the cache line. Therefore, even if a cache line having the data saved in the buffer unit as a cache entry is rewritten by a cache fill, all successive accesses within the range of the saved one cache line data can be brought into the cache hit state. .(13) When the buffer section is used, the flag control means includes: a third part assigned to selection of an access unit area of a cache line in an access address signal output by the access control means; And when the comparison result is in a cache hit state and the third part of the access address signal indicates an area other than the last access unit area of the cache line, the flag means is set to the first state; The flag means may be set to the second state when the comparison result indicates a cache miss state or the third portion of the access address signal indicates the last access unit area of the cache line.(14) To set the flag means to the first state in consideration of the cache fill also in the above (13),  The flag control means further performs a cache fill operation in response to the comparison result being a cache miss state, and writes (registers) the contents of the fill to the memory unit and the buffer unit, and sets the cache miss state. When the third portion of the access address signal indicates an area other than the last access unit area of the cache line, the flag means may be set to the first state.In some cases, it is necessary to take into account a multi-process in which a plurality of programs are executed while being switched while controlling the flag means. In the case of the multi-process, when the process switching occurs, the validity of the flag means set to the first state (set state) before the process switching cannot be guaranteed, and there is a possibility of malfunction. In order to cope with this, for example, as a simple means, the central processing unit outputs a signal indicating the process switching when the process is switched, and on condition that the process is switched by this signal,  The flag means can be in the second state.(15) The access control means may be a central processing unit provided on the same semiconductor substrate together with the cache memory means. The access control means may be digital signal processing means provided on the same semiconductor substrate together with the cache memory means.(16) A data processing system using the data processing device includes the data processing device, memory means accessed by the data processing device, and peripheral circuits controlled by the data processing device. Is done. According to this data processing system, both low power consumption and high-speed memory access can be realized.(17) The case where the present invention is applied to a cache memory has been described above. However, it is easily understood that the present invention is also applicable to a data processing device having a memory (RAM, ROM) system of a preceding operation type. Will be appreciated.[0035]DESCRIPTION OF THE PREFERRED EMBODIMENTS << Microcomputer >> FIG. 2 shows a block diagram of a microcomputer (single-chip microprocessor, single-chip microcontroller) as an example of a data processing device according to the present invention. Microcomputer (MPU) shown in the figure  1 is formed on one semiconductor substrate (semiconductor chip) such as single crystal silicon by, for example, a known semiconductor integrated circuit manufacturing technique. Although not particularly limited, the microcomputer 1 includes a local bus L-bus, an internal bus I-bus, a peripheral bus P-bus, and the like. These buses are provided with data, address, and control signal lines.The local bus L-bus includes, but is not limited to, a central processing unit (CPU) 101, a digital signal processor (DSP) 3, a cache memory (CACHE) 4, and a clock pulse generator (CPG) 14. Is done. The cache memory 4 is coupled on the other hand to an internal bus I-bus, which has a write-back buffer (WBBUF).  6 and a bus controller (BSC) 7 are connected. The bus controller 7 is connected to an external input / output circuit (EXIF) 9 and the peripheral bus P-bus. The external input / output circuit 9 can be interfaced with an external bus EX-bus having signal lines for address, data and control signals. An external memory (MMRY) 20 is representatively shown on the external bus EX-bus. In the peripheral bus P-bus, as a peripheral module, for example, a direct memory access controller (DMAC) 8 and another peripheral circuit (PMD) 10  Are combined.The microcomputer 1 is operated in synchronization with a clock signal 15 output from a clock pulse generator (CPG) 14. The CPU 3 and DM  AC8 constitutes a bus master module. The other peripheral circuits 10 include, but are not limited to, a serial communication interface controller, a real-time clock circuit, and a timer circuit. The peripheral circuit 10 is accessed by the CPU 101 or the DMAC 8 via the bus controller 7.Although not particularly limited, the CPU 101 controls an arithmetic unit represented by a general-purpose register or an arithmetic and logic unit, a control register group such as a program counter, and fetches and decodes an instruction and controls an instruction execution procedure. An instruction control unit for performing arithmetic control is provided. The CPU  101 fetches an instruction from the external memory 20 or the like, and decodes the instruction with an instruction decoder to perform data processing according to the instruction.The digital signal processor 3  Is connected via its dedicated buses X-bus and Y-bus.  Memory (XMEM) 11 and Y memory (YMEM) 12  Connected to. The X memory 11 and the Y memory 12 are also interfaced to the internal bus I-bus. The memory controller (MCNT) 13 includes the X memory 11 and the Y memory 1  Access request from DSP 3 to internal bus I-  It monitors access requests from the bus side and arbitrates access requests. The memories 11 and 12 are a CPU 1  01 is also available as a work area. CP  U101 not only fetches data for DSP3, but also fetches all instructions for DSP3, including fixed point instructions.The bus controller 7 includes a CPU 101  The bus cycle is controlled by controlling access data size, access time, insertion of a wait state, and the like in accordance with a circuit to be accessed by the DMAC 8 (address area to be accessed).<< Generation of CPU Address >> FIG.  The configuration of an address generation unit 101 is schematically shown. The CPU 101 has an address generation unit 201 and a data processing unit 202. The address generation unit 201 generates an address signal to be output to the address bus 107. The data processing unit 202 includes an instruction control unit that decodes a fetched instruction to generate a control signal, and an arithmetic circuit that performs an operation using the fetched operand and the like.The address generator 201 generates an address signal for instruction fetch and an address signal for data access. Although not particularly limited, for convenience, an instruction fetch is performed by a normal instruction fetch (NIF (= Normal Inst. Fetc).  h)) and forced instruction fetch (FIF (= Forced Inst. Fet)  ch)).The normal instruction fetch (NIF) is a case where the current instruction fetch is performed from an address obtained by adding (incrementing) a predetermined amount to the instruction address fetched last time. In the figure, what is indicated by 204 is a program counter (PC), and the output of the program counter 204 is selected by the selector 203 and output to the address bus 107 at the time of normal instruction fetch. In parallel with this, preparation for the next normal instruction fetch is performed. That is, as illustrated in FIG. 4, an address for the next normal instruction fetch is generated by the increment circuit (INC) 205. The output of the increment circuit 205 is selected by the selector 206 and stored in the program counter 204. Thus, it is prepared for the next normal instruction fetch.The forced instruction fetch (FIF) is a case where the present instruction fetch is performed from an instruction address that is not directly related to the instruction address fetched last time. For example, there is a case where a branch destination instruction when a branch instruction or the like is executed is fetched. At the time of the forced instruction fetch, as shown in FIG. 5, instead of the output of the program counter 204, a value generated by another address calculation means (not shown) is given as an address from the path 207, and the selector 2  03 is selected and output to the address bus 107. In parallel with this, the selector 206 selects the path 207 for the next normal instruction fetch at the branch destination, and stores this in the program counter 204.At the time of data access (DA), as shown in FIG. 6, a value generated by an address calculating means (not shown) is given as an address from a path 208, selected by a selector 203, and selected by an address bus 107. Is output toThe bus command (BCMD) 104 is  As illustrated in FIG. 7, the non-operation (NO  P), data access (DA), normal instruction fetch (N  IF) and a forced instruction fetch (FIF). The CPU 101 determines the bus command 104 according to the type of instruction to be executed.As is clear from the above description, in the normal instruction fetch (NIF), there is a certain relationship between the current instruction fetch address and the previous instruction fetch address, such as the number of address increments. In other words, when the bus command means the normal instruction fetch, the bus command means that the current access address signal is continuous with respect to the previous access address signal. As described above, continuous means that the number of address increments is a specified value. The bus command 104 is an example of a first signal. Focusing on this, in the normal instruction fetch, it is possible to identify the current instruction fetch address with respect to the previous instruction fetch address as long as the normal instruction fetch can be recognized without looking at the instruction fetch address signal. it can. The CPU 101 outputs the bus command 104 at the time of memory access, and externally recognizes that the instruction is a normal instruction fetch by the bus command 104.In the control of the cache memory 4, whether or not the current access address is continuous with respect to the previous access address is determined from the bus command 104, and a cache hit on the same cache line can be continued. By pre-determining the performance from the configuration of the cache memory, it is possible to prefetch a cache hit without performing an address comparison (without operating an address array) and reduce the number of operations of the address array and the data array. I have. This function is called a low power fetch (LPF) and will be described in detail below.<< First Example of Cache Memory >> FIG. 1 shows a first example of the cache memory 4. Although the cache memory 4 shown in FIG.  It is a 4-way set associative type instruction cache memory.The cache memory 4 stores the address array 1  09, data array 110 and cache control circuit 32  It is roughly divided into two. The address array 109 has memory blocks 115 to 118 corresponding to the four ways, and the data array 110 has memory blocks 119 to 122 corresponding to the four ways. The first way is composed of memory blocks 115 and 119, the second way is composed of memory blocks 116 and 120, and the third way is  The fourth way is constituted by memory blocks 117 and 121, and the fourth way is constituted by memory blocks 118 and 122.In FIG. 1, reference numeral 107 denotes an address bus, 10  8 is a data bus. The address signal supplied from the CPU 101 to the address bus 107 is, from the viewpoint of the operation of the cache memory, a tag address,  It can be regarded as an index address and an offset address.Memory block 1 of address array 109  Numerals 15 to 118 have storage units (called cache tag storage units) for storing tags (cache tags) to be compared with the tag addresses, valid bits indicating the validity of cache entries, and the like.Memory block 11 of data array 110  Nos. 9 to 122 have cache lines corresponding one-to-one with the cache tag storage unit. FIG. 8 shows details of one memory block 119 as a representative. Although not particularly limited, the cache line 610 has a data storage area of 16 bytes. The cache line 610 stores data at an address corresponding to the cache tag. The cache tag storage unit and the corresponding cache line can be commonly selected by the same index address. According to this example, the instruction word length is a maximum of 4 bytes. The cache line is selected by the address decoder 608 receiving the index address 607. The selector 611 selects 4 bytes from the 16-byte data of the selected cache line. The offset address 609 is used for the selection. The unit of selection by the selector 611 is 4 bytes,  Since the address signal is a byte address, the offset address 609 is the information of the third bit (CAB [2]) and the fourth bit (CAB [3]) from the lowest order of the address signal output by the CPU 101. Other memory blocks in the data array are similarly configured.Memory block 1 of address array 109  15 to 118 are commonly controlled by a signal 106 to enable or disable (active / inactive). When the memory blocks 115 to 118 are activated, the cache tag in the cache tag storage unit selected by the index address is compared with the tag address on the upper side of the index address. The comparison operation is performed by the comparators 115c-1  18c performs. The comparison results by the individual comparators 115c to 118c are the way hit / miss signals 115h to 118h.  (This is collectively referred to as a cache hit / miss signal 131). Cache hit / miss signal 131  Are temporarily stored in the latch 123 and output as the cache hit / miss signal 132. The signal 115h  A state in which one of .about.118h indicates a match (way hit) is called a cache hit state, and a state in which all of them do not match (way miss) is called a cache miss state.Memory block 11 of data array 110  9 to 122, the activation / inactivation of the operation is individually controlled by signals 305 to 308. The data of the cache line selected by the index address and the offset address are stored in the corresponding latch circuits 124 to 127.  Is temporarily held. The outputs of the latches 124 to 127 are selected by the selector 111 by the signal 321, and the selected data 114 is sent to the CPU 10 via the data bus 108.  1 is supplied.The selection signal 321 of the selector 111 is the cache hit / miss signal 132 or the signal 317 output from the latch 123. Either one is selected by the selector 304, and the selected signal is the selection signal 321. You.The signal 317 is a low power fetch (L  PF) module 301. The cache control unit 322 controls the entire cache memory including the low power fetch module 301. The CPU 101 starts the memory access operation after the ready (bus ready) signal 103 is asserted. The cache control unit 322 sends the bus command 104 from the CPU 101.  And thereby recognizes the start of the operation of the normal instruction fetch or the forced instruction fetch by the CPU 101.  As a result, the cache control unit 322 starts the associative search operation, and outputs necessary instructions to the data bus 108. C  The PU 101 fetches an instruction on the data bus 108.The cache control unit 322 performs control according to a cache hit state / cache miss state,  In the cache hit state, the address array 109  And a normal function for operating the data array 110 in parallel, and an LPF function. Although details of the circuit configuration related to the case of the cache miss state are not shown in FIG. 1, an outline thereof will be described here. If the result of the associative search for the cache entry is a cache (read) miss state, the cache memory 4 reads one cache line of data including the data relating to the miss from the external memory 20 via the bus controller 7. ,  A cache fill is performed. The cache fill is an operation of reading the data of the cache line from the external memory 20 or the internal memory (IMEM) 20B. In order to write the read data to the cache line, the cache entry is replaced. At this time, if there is an invalid cache entry, the invalid cache entry is replaced. When there is no invalid cache entry, for example, LRU (Least Recently)  The cache entry that has not been used most recently is subject to replacement according to a logic such as Used. The replacement control is performed by the cache control unit 322.Next, the LPF function will be described in detail. LPF  The function is that, first, it is possible to detect a state in which a cache hit state occurs in the same cache line without going through an address comparison operation. At least two points that can be specified without intervention. If it is a direct map format, only the first need be considered.FIG. 9 shows an example of the LPF module 301.  In the above, an LFP flag 313 is provided to achieve the first point, and a latch 314 and a selector 315 are provided to realize the second point. Further, control logic (FIG. 10) for setting the LPF flag 313 to a set state (first state) and control logic (FIG. 1) for setting the LPF flag 313 to a reset state (second state).  1) The control logic (FIG. 12) of the latch 314 and the selector 315 is provided in the cache control unit 322.The LPF flag 313 is constituted by, for example, a set / reset type flip-flop.  The state signal (LPFV) 316 of the LPF flag 313 is set to a high level ("1") in the set state and to a low level ("0") in the reset state.The set control logic 400 constituting the flag control means is illustrated in FIG. The NAND gate G1 is  It detects whether the address of the next normal instruction fetch (NIF) is the same cache line as this time. As described in FIG. 8, if the offset addresses CAB [3] and CAB [2] are other than 1, 1, the data specified by the address of the next normal instruction fetch is in the same cache line as this time.  OR gate G2 has way hit / miss signals 115h-1  Input 18h to detect the hit state of the cache memory. That the cache hit state is at the time of the instruction fetch operation is determined by the second bit BCMD of the bus command.  The fact that [1] is a logical value "1" may be detected by the AND gate G3. The high level output of the AND gate G3 indicates that the instruction fetch is in a cache hit state.  As a result, the set signal 309 is in a cache hit state when the LPFV 316 is at a low level and the offset address of the access address signal at that time indicates other than the last 4 bytes of the cache line, or  When the cache fill operation is performed in response to the cache miss state and the offset address of the access address signal related to the cache miss state indicates an area other than the last 4 bytes of the cache line, the set state (first state) To be.The reset control logic 401 constituting the flag control means is exemplified in FIG. AND gate G4  Detects whether data specified by the address of the next normal instruction fetch is in a cache line different from the current cache line. CAB of offset address as explained in FIG.  If [3] and CAB [2] are 1,1, the cache line that is brought into the cache hit state in the next normal instruction fetch is a cache line different from the current cache line. The NOR gate G5 receives the way hit / miss signals 115h to 118h and detects a cache miss state of the cache memory. Whether the cache miss state is at the time of the instruction fetch operation may be detected by the AND gate G6 detecting that the second bit BCMD [1] of the bus command is a logical value "1". A high-level output of the AND gate G6 indicates that a cache miss has occurred in the instruction fetch. This allows  The reset signal 309 indicates a reset state (second state) when the address comparison result indicates a cache miss state, when the offset address of the instruction fetch address indicates the last 4-byte area of the cache line, or when a reset is instructed to the cache memory. ).In FIG. 12 showing the control logic 402 of the latch 314 and the selector 315, reference numeral 311 denotes a selection signal of the selector 315, and reference numeral 312 denotes a latch control signal of the latch 314. Reference numeral 320 denotes a selection signal (a cache fill way selection signal) for a way that has been subjected to cache fill due to a cache miss state in instruction fetch. The cache fillway selection signal 320 is output from the cache control unit 322. The selection signal 311 is set to a high level when the instruction fetch is in the cache hit state, selects the way selection signal 132 by the address comparison operation, and selects the selected way selection signal 1  32 is held by the latch 314. When a cache fill due to a cache miss state at the time of instruction fetch is performed, the selection signal 311 is at a low level.  , And the selected signal is held in the latch 314. Obviously, the latch operation of the way selection signal in the cache hit state and the way selection signal in the cache fill is performed in parallel with the setting operation of the LPF flag 313 described in FIG.Accordingly, the set control logic 400 and the reset control logic 401 which constitute the flag control means allow the LPFV 316 to execute the next normal instruction fetch.  It is possible to indicate a state in which a cache hit occurs in the same cache line without going through the address comparison operation.  At this time, information for selecting the way to be in the cache hit state is latched by the latch 314.In the set control logic 400 of FIG. 10, the LPF flag 313 is set in consideration of the cache fill caused by the cache miss state in the instruction fetch, but it may be excluded from the set factors. In such a case, the LPF flag 313 is set in the next consecutive instruction fetch following the cache fill operation. Similarly, in that case, the latch operation of the latch 314 becomes unnecessary in the instruction cache fill operation in FIG.The cache control unit 322 uses the bus command 104 from the CPU 101 and the state of the LPFV 316 to operate the cache memory 4 without performing address comparison using the way selection information latched by the latch 314. judge. The determination control logic for that is shown in FIGS.FIG. 13 shows the selection logic 403 of the selector 304. When the LPFV 316 is in the set state and the bus command 104 indicates the normal instruction fetch (NIF), the selector 304 selects the way selection signal 317 held by the latch 314, otherwise, the actual address is selected. The way selection signal (way hit / miss signal) 132 obtained by the comparison is selected. As is apparent, in the former case, the operation of the address array 109 is unnecessary.FIG. 14 shows an operation control signal 106 common to each of the memory blocks 115 to 118 of the address array 109.  And each of the memory blocks 119-1 of the data array 110  At 22 is shown the control logic (404) of the individual operation control signals 305-308. In the figure, the OR gate G1  0 indicates data access (DA), normal instruction fetch (NI  F) or a high level is output in forced instruction fetch (FIF). The AND gate G11 outputs a high level at the time of normal instruction fetch (NIF). The AND gate G12 outputs a high level when the LPFV 316 is in a set state (high level) and in a normal instruction fetch (NIF).Therefore, the operation control signal 106 of the address array 109 is set to the low level when the LPFV 316 is in the set state (high level) and the normal instruction fetch (NIF), thereby suppressing the operation of the address array 109. Data access (DA), LPFV31  Normal instruction fetch (NIF) in reset state of No. 6  Alternatively, in the forced instruction fetch (FIF), the address array 109 is made operable and performs an address comparison operation.The operation control signals 305 to 308 for the memory blocks 119 to 122 of the data array 110  When the LPFV 316 is in a set state (high level) and is in a normal instruction fetch (NIF), the latch 314  Selection signal 317 (LPFWAY [0], LPFW  AY [1], LPFWAY [2], LPFWAY [3]) enable only the memory block corresponding to the way selected. Otherwise, all the memory blocks 119 of the data array 110  ~ 122 are enabled in parallel.As is clear from the above, the LPF flag 3  13 (LPFV 316) can indicate a state in which a cache hit occurs on the same cache line without going through an address comparison operation in the next normal instruction fetch by the set state. At this time, information for selecting a way to become a cache hit is latched by the latch 314. LPF flag 313 is set (LPFV  = 1) and the next instruction access is a normal instruction fetch (NI  F) (this state is called an LPF hit state), the operation of the address array 109 is suppressed by the control logic 404 in FIG. The operation is selected using the way selection information of the latch 314, and the data read from the selected memory block is also selected using the way selection information of the latch 314 and supplied to the CPU 101.FIG. 15 shows the flow of signals in the LPF hit state by thick lines. In the LPF hit state, the address array 109 is not operated and the data array 110 is not operated.  It is sufficient to operate only one way or memory block. It is assumed that the LPF flag 313 is set to 1 (LPFV = 1), and a value indicating selection of the first way is set in the latch 314.When the CPU 101 instructs the cache control circuit 322 to perform the NIF access by the bus command 104 and outputs the address to the address bus 107, the cache control circuit 322 sets the LPFV 316 to the set state (= 1) and sets the bus command 104 to the NIF access. Is detected, the LPF hit state is recognized. Thereby, the memory block 119 of the first way indicated by the way selection signal 317 is operated. At this time, the operation instruction signal 3 of another memory block of the data array 110  06 to 308 and the operation control signal 106 of the address array 109 maintain a negated state. Also, the selection signal 30  3, the selector 304 selects the way selection signal 317. As a result, the output of the memory block 119 of the first way is selected as the output of the data array 110.  The instruction code read from the memory block 119 is temporarily held by the latch 124, selected by the selector 111, and passed to the CPU 101 via the data bus 108. Thus, at the time of the LPF hit, only one memory block 119 of the data array 110 needs to be operated.  Other memory blocks are not operated. As a result, wasteful power consumption is suppressed. In addition, since the control is not based on a simple expectation such as simply remembering the index address of the previous access and comparing the index address with the current index address, no error occurs in the determination result based on the LPFV 316 and the bus command 104, and the address is re-entered. It is possible to completely prevent a delay in the data read operation without causing a situation in which the determination is restarted.FIG. 16 shows the signal flow in the LPF miss state by a thick line. For example, LPF flag 31  It is assumed that 3 (LPFV 316) is reset to 0. In this state, the CPU 101 sets the bus command 1  04 indicates NIF access, LPFV =  Since it is 0, the cache control unit 322 detects an LPF miss state. As a result, the cache control unit 322 asserts the operation control signal 106 and the address array 1  09 is operated. The cache control unit 322 asserts all the operation control signals 305 to 308 in parallel with it, and all the memory blocks 119 of the data array 110 are asserted.  １２２122 are enabled. Also, the cache control unit 3  Reference numeral 22 indicates a signal 13 to the selector 304 by the control signal 303.  Select 2. Assuming that a cache hit occurs in the first way due to the index operation and the address comparison operation in the address array 109, of the cache line data indexed in each of the memory blocks 119 to 122 of the data array 110, the data is transmitted from the memory block 119 in the first way. The read instruction is used as the selector 11 by the selection signal 132 based on the cache hit signal 131.  1 is selected. The instruction code selected in this way is passed to the CPU 101 via the data bus 108. When not in the LPF hit state, although the power consumption is useless compared to the LPF hit state, a data read speed equivalent to a format in which the address array and the data array are operated in parallel can be maintained.The cache fill operation for the cache memory 4 is assumed to be a blocking cache. That is, the bus ready signal 103 is sent to the CPU 101 only after the cache fill of one cache line is completed.  Assert Even in the case of the non-blocking method, L after the cache fill for one cache line is completed.  By setting the PF flag 313, the same effect can be obtained. This means that, for a 16-byte instruction continuously stored in one cache line 610 in FIG. 8, if the first 4-byte instruction is in a cache hit state, the remaining 12-byte instruction is always in a cache hit state. This is to ensure that it is possible.Next, the operation of the cache memory 4 will be described with reference to the timing charts of FIGS.FIG. 17 shows an example of operation timing when a continuous instruction fetch is performed in a cache memory that does not employ the LPF function. In the figure, the WAY0 address array to the WAY3 address array mean the memory blocks 115 to 118 of the address array 109.  The AY0 data array to the WAY3 data array mean the memory blocks 119 to 122 of the data array 110. At time 1, it is assumed that the bus command is NIF and the address on the address bus is H'00. As a result, all of the address array 109 and the data array 110 operate as a memory, the hit way is determined by the address comparison operation in the address array, and the instruction indexed by the way in the hit state is selected and output to the data bus. Although the NIF continues until time 9, all the address arrays 109 and data arrays 110 are operated each time.FIG. 18 shows an example of the operation when NIF accesses resulting in LPF hits continue. At time 1, the bus command is NIF and the address on the address bus is H'00. In this state, the address array 109 and the data array 110 are all operating. Because this is the head of the cache line, the relationship with the cache line of the previous instruction fetch is not known. For example, when it is determined that an instruction required for the first way is registered by the address comparison operation based on the read data of the address array 109, the instruction code is changed from the first way (WAY 0 data array) of the data array 110 at time 2. Read to data bus. Information that a hit has occurred in the first way is stored in the latch 314. Further, the LPF flag 313 is set to prepare for the next NIF access.At time 2, the bus command is NIF and the address on the address bus is H'04. This address is an address obtained by adding 4 bytes to the previous access address. Since the LPFV 316 is in the set state, the value (LPFW) held by the latch 314  The target instruction code is stored in the first way indicated by AY). The memory block 119 corresponding to the first way of the data array 110 is operated based on the instruction signal of the first way held by the latch 314, and the target instruction code is read from the memory block 119.  At this time, the address array 109 and the data array 11  The operation of the other memory blocks 120 to 122 of 0 is unnecessary. In the figure, LPFV = 0 at time 4 and time 8. This is because the LPFV was reset to correspond to the last 4 bytes of the cache line. In the figure,  The shaded portion means an operation unnecessary portion. Compared to FIG. 17, the operation of a part of the address array 109 and the data array 110 can be stopped. The operation is completed at times 1 to 9 in response to the access request from the CPU 101, as in the case of FIG. That is, a time lag of data reading does not occur in FIG.FIG. 19 shows an example of an operation in which a cache miss state and a cache fill occur when NIF accesses resulting in LPF hits are continued. A cache miss occurs in the NIF access at time 1 and, at time 2, 3, 4, and 5, a cache fill to the data array WAY0 is performed. Here, the write data to be cache-filled is read every four bytes. At time 5 when the last data is registered, LP  The F flag 313 is set (LPFV = 1),  At the same time, the instruction information of the first way information is registered in the latch 314. In the NIF access after the registration is completed, the cache hit state is continuous, and the memory operation is performed by the data array 11 necessary for reading the instruction, as in FIG.  A first way of 0 is sufficient.FIG. 20 shows an example of the operation in the case where the FIF access is successively a cache hit. In this case, since the instruction is fetched by FIF access, the LPF function is not used at all. In the FIF access, the current instruction address cannot be obtained based on the previous instruction fetch address. However, even in this case, the power consumption and the data read speed (the time from the address output by the CPU 101 to the instruction read) are as shown in FIG.  Same as, but not worse.<< Second Example of Cache Memory >> Next, a case where the cache memory 4 is configured as a cache memory (unified cache memory) that stores instructions and data in a mixed manner will be described.Unlike the instruction cache memory of the first example, in the instruction / data mixed cache memory, it is necessary to consider a case where a cache entry of an instruction is evicted from a cache line of interest due to a cache fill of data. Must. The simplest solution is  When a data cache fill occurs,  What is necessary is just to reset the PF flag 313. However, in this case, even if the cache line to be cache-filled for data is not the cache line during the instruction fetch, L  Even if the PF flag 313 is reset and the LPF hit state can be obtained immediately after that, the LPF miss state occurs, and the effect of low power consumption is diminished in this regard. In the following description, an example will be described in which it is detected whether or not a cache entry of an instruction has been evicted from a cache line of interest by a cache fill of data.FIG. 21 is a block diagram of the cache memory 4A configured as the unified cache memory. The difference from FIG. 1 is that the LPF module 301  A and the configuration of the cache control circuit 322A. FIG.  Circuit blocks and signals having the same functions as those described above are denoted by the same reference numerals, and detailed description thereof is omitted.FIG. 22 shows an example of the LPF module 301A. The LPF module 301A is the LPF  A latch 1108 and comparators 1109 and 1112 are provided in addition to the flag 313, the latch 314, and the selector 315. The latch 1108 latches the index address 607. The comparator 1109 compares the value held in the latch 1108 with the index address 607,  If they match, the signal 1110 is set to high level. The comparator 1112 outputs the way selection signal held in the latch 314 and the way selection signal 3 at the time of the cache fill.  20, and when they match, the signal 1113 is set to the high level.The generation logic of the signals 311 and 312 is illustrated in FIG. The logic shown in the figure is the same as the logic 402 described in FIG. The difference from FIG. 12 is that the latch operation of both latches 314 and 1108 is controlled by the signal 312. Therefore, when the instruction fetch is in the cache hit state, the latch 314 holds the index address related to the cache hit, and the latch 1108 holds the way selection signal related to the cache hit state. In the cache fill state caused by the cache miss state of the instruction fetch, the latch 314 latches the way selection signal related to the cache fill, and the latch 1108 latches the index address related to the cache fill.As described above, the way selection information and the index address information latched by the latches 314 and 1108 are values related to the instruction fetch or the instruction cache fill. Therefore, in a cache fill operation caused by a cache miss state in data access, an index operation is performed using the same index address as the value held in latch 1108, and the same way as the way selection information held in latch 314 is used. The selected state can be determined from the output signals 1110 and 1113 of the comparators 1109 and 1112.If the index address and way of the cache line used in the cache fill due to the cache miss state of the data access are the cache line in which the instruction is registered, the instruction is evicted from the cache line. . As is clear from the above, in this case, the signals 1110 and 1113  Is set to a high level. If the instruction fetch is continued after the occurrence of this state, a malfunction may occur. To avoid this, the LPF flag 3  Thirteen reset conditions include the signals 1110 and 111  Please refer to FIG.That is, in the reset control logic 401A shown in FIG. 24, when the LPFV 316 is set and the cache fill state occurs, the state in which the signals 1110 and 1113 are set to the high level is ANDed. Detected by gate G20 and AND gate G2  The LPF flag 313 is reset by reflecting the high level output state of 0 in the reset signal 310. FIG.  4 is the same as that of FIG. The set signal 309 of the LPF flag 313 is generated by the same logic as the set control logic 400 described with reference to FIG.As a result, the LPF flag 313 can be set by the cache fill at the time of instruction access, and when the instruction cache line of interest is evicted by the cache fill at the time of a cache miss in data access, the comparison operation of the access way and the index address is performed. , The LPF flag 313 can be reset.Next, the unified cache memory 4  The operation of A will be described. FIG. 25 shows an example of operation timing when the instruction code of the first way is not evicted from the cache line due to a cache fill caused by a cache miss at the time of data access. At time 1, NIF access is issued at address H'000.  On the other hand, a cache hit state is set, and at time 2, an instruction code is output to the data bus. This gives L  The PF flag 313 is set (LPFV = 1), and the first way and the latch 110 are set in the latch 314 (LPFWAY).  8 is set to the index address H'00.At time 2, the address H'004 is  Is required. Since the LPF flag 313 is set at the time 1, only the first way of the data array 110 is operated.At time 3, data access is requested to address H'800. At this time, the memory blocks operate on all of the address array 109 and the data array 110, but a cache miss has occurred. Therefore, a cache fill is performed, and the target of the cache fill is, for example, the second way.At times 4 to 7, the cache fill operation for the second way of data array 110 is completed. At this time, since the latch 314 holds the information of the first way, the LPF flag 313 is not reset until the time 7 and the set state (LPFV = 1) is maintained. That is, the currently fetched instruction code in the first way of the index address H'00 still exists on the cache line.At time 8, NIF is specified at address H'008 for the instruction fetch following time 2. At this time, the LPF flag 313 is in the set state and the latch 3  The first way of the data array is operated according to the information of No. 14, and the required instruction code is read according to the index address.At time 9 following this, only the first way of the data array is operated for the NIF to the address H'00C, and the instruction code is read. L  The PF flag 313 is reset (LPFV = 0).  In the next instruction fetch, the access address is H'00C + H'00  This is because 4 = H'010 and another cache line is indexed. Whether or not another cache line causes a cache hit is determined again by the address array 10.  It is necessary to operate 9 to check.FIG. 26 shows an example of the operation timing when the instruction code of the first way is evicted from the cache line by the cache fill caused by the cache miss state at the time of data access.The operations at times 1 and 2 are the same as in FIG.  At time 3, data access to H'800 is requested,  Assume that a cache miss has occurred. Then, it is assumed that the target of the cache fill resulting therefrom is the first way, and has the same index address as the immediately preceding instruction access. A cache fill is performed for the first way at times 4 to 7. In this case, the LPF flag 313 is reset (LPFV = 0). In this state, since the data has been cache-filled in the cache line in which the instruction being fetched is registered, necessary instructions have been evicted from the cache. Therefore, the NIF access to H'008 at time 8 does not result in an LPF hit, and the cache memory operation is performed by operating all the memory blocks of the address array and the data array. In this case, as a result, a cache miss state occurs, and a cache fill of the instruction related to the cache miss is performed from time 9 to time 12. In this example, the cache fill is performed in the third way.According to the second example of the cache memory described above, the LPF is  The flag 313 can be set, and when the instruction cache line of interest is evicted due to a cache fill at the time of a cache miss in data access, the LP is accessed via the operation of comparing the access way and the index address.  The F flag 313 can be reset. Therefore, also in the unified cache memory, as in the first example, the address array 109 and the data array 110  And low power consumption can be realized while maintaining a data reading speed equivalent to that of a cache memory of a type that operates in parallel.<< Third Example of Cache Memory >> FIG. 27 is a block diagram of a cache memory in which power consumption at the time of an NIF hit is further reduced. The cache memory 4B shown in FIG. 27 is different from the configuration of FIG.  10 and the cache control unit 322B are different.The data array 110 includes the four memory blocks 119 to 122 and the individual memory blocks 11 to 122.  The cache line buffers 119b to 122b correspond one-to-one with the cache lines 9 to 122. The cache line buffers 119b to 122b correspond to the corresponding memory block 1  The cache line data indexed by 19 to 122 is internally transferred. Memory blocks 119 to 12  2 are enabled together by signal 1502.  The internal transfer of the cache line buffers 119b to 122b is performed by the way hit / miss signals 115h to 115h.  8h and the set signal 309. The signal 303 indicates which of the outputs of the memory blocks 119 to 122 or the outputs of the cache line buffers 119b to 122b is to be selected.FIG. 28 is a typical block diagram of one memory block 119 and cache line buffer 119b. The configuration of the memory block 119 is substantially the same as that of FIG. However, the activation control of the memory block 119 is determined by a control signal 1502 common to the other memory blocks 120 to 122. Reference numeral 620 denotes a memory cell array forming a plurality of cache lines 610.The cache line buffer 119b has the same storage capacity as one cache line (1 according to this example).  A line memory array 621 having 6 bytes) and a selector 622 for selecting data from the line memory array 621 in 4-byte units. The selector 622 is the same as the selector 611 of the memory block 119, and the lower two bits (CAB [3], CAB [  According to [2]), a selecting operation is performed in units of 4 bytes. The outputs of the selectors 611 and 622 are selected by the selector 623, and the selected 4-byte data is supplied to the latch 124.The data input operation of the line memory array 621 includes the way hit / miss signal 115h and the signal 30.  It is indicated by the high level of the logical product signal of the logical AND with the signal 9. That is, the address comparison of the corresponding way is determined as a way hit, and the LPF flag 313 is set (LP  When FV = 1), the data of the cache line indexed by the memory block 119 is internally transferred to the cache line buffer 119b and held. During the instruction fetch for the 16-byte instruction held in this way, the operation of the memory block 119 is suppressed, and the necessary instruction is transferred from the cache line buffer 119b to the CPU1.  01.The flag set control logic 400B for that purpose  Are as shown in FIG. This configuration is shown in FIG.  The only difference is that the instruction cache fill state is not used as a set factor. Memory block 119-  Regardless of the way in which 122 is rewritten, the required cache line instruction is stored in cache line buffer 119b-  122b. The flag reset logic is exactly the same as the logic 401 in FIG. still,  Although illustration is omitted, if writing to the cache line buffer is performed at the time of the cache fill, the LPF hit state can be obtained at the first NIF after the cache fill is completed by setting the LPF flag 313 at the time of the cache fill.FIG. 30 shows activation control signal 106 for memory blocks 115 to 118 in address array 109.  And memory blocks 119 to 122 of data array 110  The generation logic 404B of the activation control signal 1502 is shown. 14, the logic 404B for generating the activation control signal 106 is the same as the configuration in FIG. Activation control signals 1502 for memory blocks 119 to 122 of data array 110 are also generated with the same logic as signal 106. Therefore, activation control signals 106 and 1502  Indicates that the LPF flag 313 is set (LPFV =  1) and at the time of the normal instruction fetch (NIF), it is set to the low level, whereby all the memory blocks 115 to 122 of the address array 109 and the data array 110 are set.  Operation is suppressed. Data access (DA), LPF  In the normal instruction fetch (NIF) or the forced instruction fetch (FIF) in the reset state of the flag 313, the index operation is enabled in all the memory blocks 115 to 122.The other configuration is the same as that of the cache memory 4 of the first example described with reference to FIG.In the cache memory of the third example,  The LPF flag 313 can indicate a state in which a cache hit state occurs in the same cache line without going through an address comparison operation in the next normal instruction fetch, depending on the set state. At this time, information for selecting a way to become a cache hit is latched by the latch 314. LPF flag 313 is set (LPFV  = 1) and the next instruction access is a normal instruction fetch (LPF hit), the control logic 404B of FIG. 30 suppresses the operation of all the memory blocks 115 to 122 of the address array 109 and the data array 110. . Instead, the selector 6 receiving the signal 303  23 selects the data internally transferred to the cache line buffer 621 in the preceding instruction cache hit, and the selected data is selected by the selector 111 using the way selection information 317 of the latch 314 and  1 is supplied.FIG. 31 shows an example of the operation timing in the LPF hit state at the time of NIF continuous access in the cache memory according to the third example. Since the offset address related to the NIF access request at time 1 indicates the head of the cache line, all the memory blocks 115 to 122 of the address array 109 and the data array 110 are operated. At this time, it is assumed that the first way (WAY0) has been hit. In that case, the data of the cache line indexed by the memory block 119 is stored in the CPU 10.  Given to one. In parallel with this, the instruction of the cache line indexed in the first memory block 119 related to the way hit is transmitted to the cache line buffer 119b.  Stored in The latch 314 latches the way selection information indicating the selection of the first way selected by the address comparison operation at that time, and the LPF flag 3  13 is set.Since the LPF flag 313 has been set (LPFV = 1), the signal 10  6, 1502 are negated, and the operations of all the memory blocks 115 to 122 are suppressed. Instead,  A necessary instruction is selected from the first way cache line buffer 119b designated by the latch 314, and the CPU  101. At time 4, since the last line of the cache line is selected, the LPF flag 313 is reset (LPFV = 0). Therefore, at time 5, all the memory blocks 115 to 122 are operated again to determine the cache hit / miss. This time, it is assumed that a cache hit is made in the second way. Then, the data of the cache line indexed by the memory block 120 of the second way is supplied to the CPU 101, and the 16-byte instruction of the cache line is stored in the cache line buffer 120b.  In the next NIF access, the operations of all the memory blocks 115 to 122 are suppressed in the same manner as described above, and an instruction is given to the CPU 101 from the cache line buffer 120b.FIG. 32 shows an example of operation timing when a cache miss occurs in data access at the time of NIF continuous access in the cache memory according to the third example.Assume that a cache line in which an instruction is stored as a cache entry is subjected to a cache fill due to a data access cache miss. In this case, in the configuration described in the second example,  An NIF access request that occurs subsequently enters an instruction cache miss state, and cache-filling of the instruction related to the miss is performed. In the third example, 1 is necessary for NIF access.  Instructions for the cache line are saved in the cache line buffer. Therefore, even if the cache line having the instruction saved in the cache line as the cache entry is rewritten by the cache fill, the NI within the range of the one saved cache line data is changed.  All F accesses can be cache hits. The operation at this time will be described. The offset address related to the NIF access request at the time 1 indicates the head of the cache line, whereby all the memory blocks 115 to 122 of the address array 109 and the data array 110 are set.  Is operated. At this time, it is assumed that the first way (WAY0) has been hit. In that case, the cache line data indexed by the first way memory block 119 is provided to the CPU 101. In parallel with this, the instruction of the cache line indexed in the first memory block 119 related to the way hit is stored in the cache line buffer 119b. And the latch 31  4 latches way selection information indicating the selection of the first way obtained by the address comparison operation at that time, and sets the LPF flag 313 to the set state (LPF).  V = 1).At time 2, since the LPF flag 313 is set, both the signals 106 and 1502 are negated, and the operations of all the memory blocks 115 to 122 are suppressed. Instead, a necessary instruction is selected from the cache line buffer 119b of the first way specified by the latch 314 and supplied to the CPU 101. At time 3, data access is instructed, which is a cache miss. Assume that the target of the cache fill at this time is the first way (WAY0), and the index address is the same as at time 1. This cache fill is performed from time 4 to time 7, whereby the cache line including the instruction entry during the instruction fetch is rewritten. Even so, the instruction entry evicted from the cache line remains in the cache line buffer 119b. LP  The F flag 313 is also kept set. Therefore, when the NIF access is instructed at time 8, the operations of all the memory blocks 115 to 122 are suppressed as described above, and the instruction of the cache line buffer 119b is selected by the way selection information of the latch 314 and the CPU  101.As described above, even if a cache fill due to a cache miss due to a data access occurs in a cache line during an instruction fetch, a cache miss does not occur in a subsequent NIF access, and all memory blocks are operated. Required instructions without CPU  101 can be supplied.Therefore, according to the third example, the operation of all the memory blocks can be stopped at the time of the NIF hit, and the power consumption can be further reduced as compared with the first example. Also,  Instructions for one cache line required for the NIF are saved in the cache line buffer. Therefore, even if a cache line having an instruction saved in a cache line buffer as a cache entry is rewritten by a cache fill, all NIF accesses within the range of one saved cache line data can be brought into a cache hit state. it can.<< Address Generation by DSP >> In the above example, the CPU is used as the access control means, but it may be a DSP core. For example, the DSP (Digi  The tal Signal Processor (Digital Signal Processor) includes an instruction cache memory 2005, an X data cache memory 2006, and a Y data cache memory 2007 connected to a DSP core 2001. An X data cache memory 2006 and a Y data cache memory 2007 are provided separately to efficiently perform the product-sum operation. The cache memory 2005 to 2007  Is a large-capacity memory 2 connected via a common bus 2008  The command or data held by 009 is used recently. The configuration in FIG. 33 can be implemented as a semiconductor integrated circuit with one chip except for the large-capacity memory 2009.The DSP core 2001 has the address generation unit 19  01 and a data processing unit 1902. The instruction cache memory 2005 receives the instruction fetch address output from the address generation unit 1901 from the address bus 1905, and, in the case of a cache hit, the data bus 1908.  The instruction is given to the data processing unit 1902 via the. The X data cache memory 2006 includes an address generation unit 19  01 is received from the address bus 1906, and in the case of a cache hit,  Read data is supplied to the data processing unit 1902 via the data bus 1909, and write data from the data processing unit 1902 is supplied via the data bus 1909.  Similarly, the Y data cache memory 2007 receives the data access address output from the address generation unit 1901 from the address bus 1907, and in the case of a cache hit, supplies the read data via the data bus 1910 to the data processing unit 1902. Further, write data from the data processing unit 1902 is supplied via the data bus 1910.FIG. 34 shows an outline of the address generation unit of the DSP core 2001. The instruction address is a program counter (PC) 1911 and an incrementer (IN  C) It can be continuously generated by 1912, and is a branch destination address generated by an address calculator or the like. In a product-sum operation by a DSP, for example, a filter operation, an operation is often performed by fetching certain data sequentially and regularly. That is, the next address is often generated by adding or subtracting a fixed value from the previous access address. Considering this, the address bus 1  A circuit for generating a data access address to be supplied to the 906 includes an X address register (XAR) 1913 and an increment / decrementer (INC / DEC) 1914.  Is provided. Similarly, a circuit for generating a data access address to be supplied to the address bus 1907 includes:  A Y address register (YAR) 1915 and an increment / decrementer (INC / DEC) 1916 are provided.When the address signal is a byte address, D  If the maximum length of the SP instruction is 4 bytes, the addition number of the incrementer 1911 is set to 4. Assuming that the data size that can be processed by the DSP is 1 byte, 2 bytes, and 4 bytes, the incrementer / decrementer 1914, 19  The number of addition and subtraction by 16 can be selected from 1, 2, and 4. Although not particularly limited, the data size is specified by the DSP instruction. The data processing unit 1902 instructs the increment / decrementers 1914 and 1916 on the number of addition and subtraction according to the data size specified by the instruction.The instruction cache memory 2005 has the structure shown in FIG.  Has the configuration of the cache memory 4 according to the first example described above. Although the data cache memories 2006 and 2007 are also based on the configuration of the cache memory 4 according to the first example described with reference to FIG. 1, the setting / reset conditions of the LPF flag 313 need to be changed. This will be described later.In FIG. 34, reference numeral 1904 denotes a bus ready signal output from the cache memories 2005 to 2007, and the cache memories 2005 to 2007 can start operation when asserted. 190  Signals indicated by 3,1903A and 1903B are bus commands supplied to the cache memories 2005, 2006 and 2007, respectively. Bus command 1903  Is a bus command supplied to the instruction cache memory 2005, and should be understood to be a bus command equivalent to the bus command shown in FIG.FIG. 35 shows a data cache memory 200.  6, 2007 supplied to bus command 1903A, 1  An example is shown for 903B. Bus command 1903A,  1903B is 3 bits each, and according to the code, the access size is set to 1, 2, 2 for read access.  The method of address change is specified by increment or decrement of 4 bytes. The description of the case where the access address is not continuous and the bus command related to the write access are omitted, but it should be understood that another code is assigned. FIG.  Can be regarded as information indicating that the current access address is continuous with respect to the previous access address.FIG. 36 shows an example of the set / reset control logic of the LPF flag 313 when the bus command of FIG. 35 is considered. AND gates G50 to G55,  AND gates G60 to G65 and OR gate G66  The next NDF (normal data fetch: means a data fetch in which the access address is continuous, and corresponds to the NIF in the instruction cache memory) is 16  It is detected whether or not it is the last area of the byte cache line. For example, the gate G50 detects whether the lower 4 bits of the address signal have the logical value "1" of all the bits, in other words, whether the access is for every 16th byte. The AND gate G60 sets the lower 4 bits of the address signal to the logical value “1” of all bits and if the bus command is INC-1,  Become high level. This high level indicates that the next NDF access is 16 in the NDF continuous access of INC-1.  It means that it becomes the last area of the byte cache line. As understood from this, the OR gate G6  The high level output of 6 is one reset condition for the LPF flag 313. Other reset conditions are a cache memory reset and a cache miss. The establishment of the reset condition is reflected on the signal 310. The condition for setting the LPF flag 313 is that the cache fill state or the cache hit state is satisfied and the next N  It is determined that the DF access is within the cache line,  It is reflected in signal 309.As described above, by adapting the set / reset condition of the LPF flag 313 to the bus command of the DSP, the low power consumption of the cache memory and the memory access can be prevented even for memory access by access control means other than the CPU. Speedup can be achieved.<< Digital Still Camera >> FIG. 37 is a system block diagram of a digital still camera to which a microcomputer incorporating the cache memory is applied.The microcomputer 1 shown in FIG. 13 is a simplified version of the configuration described in FIG. The microcomputer 1 includes a serial input / output circuit (SIO) 10A and a serial FIFO (First-in First-  out: infrared interface (SCIF IrDAI / F) 10B having a first-in first-out buffer memory, serial FIF  Modem interface with SC (SCIF modemI / F) 1  0C and DMAC8. An A / D converter / D / A converter 80 is provided in the serial input / output circuit 10A.  0, the microphone 802 and the speaker 801 are connected. The system bus 803 to which the microcomputer 1 is connected includes a ROM (Read Only Memory) 804 and an SDRAM (Synchronous Dynami).  c Random Access Memory: Synchronous DRAM) 80  5. LCD (Liquid Crystal Display) controller 8  06, a compact flash memory card interface (C-Flash I / F) 807, a camera DSP 808, and the like are connected.  809 is connected to the camera DSP 808 via an A / D converter 810 and a CCD (Charge Coupled Device).  e) Camera 811 is coupled. A compact flash memory card (CF) 812 is detachably attached to the interface 807 for the compact flash memory card. Although not particularly limited, the digital still camera shown in the figure is driven by a battery power supply circuit 813. The battery power supply circuit 813 includes an operating power supply voltage V  dd and Vss.The cache memory 4 is not particularly limited, but the SDRAM 805, the ROM 804, or the compact flash memory card 812 is to be cached. As described above, the cache memory 4 achieves low power consumption for memory access to continuous access addresses. Therefore, it is possible to contribute to extension of the driving time of the system driven by the battery power supply circuit 813. Also, regardless of the LPF hit / miss, a data reading speed equivalent to that of a cache memory in which an address array and a data array are operated in parallel can be guaranteed, so that data processing performance is sacrificed even with low power consumption. It will not be done.Although the invention made by the inventor has been specifically described based on the embodiment, it is needless to say that the present invention is not limited to the embodiment and can be variously modified without departing from the gist of the invention. No.For example, the cache memory means may be realized as an address translation buffer for a virtual storage or a memory management unit. The access control means is not limited to the CPU or DSP core,  Other bus master modules such as C may be used. In this case, the present invention is regarded as a method of previously knowing the contents of the address register in the bus master module. Further, the cache memory may be set associative or a direct map. In the case of set associative, the number of ways is not limited to four, but may be other. The present invention is also applicable to a data processing device that realizes multi-process (pre-entity program, multi-thread program). Further, the data processing device including the access control unit and the cache memory is not limited to a single chip, but may be configured by a multi-chip. Further, the content of the bus command is not limited to the above example and can be changed as appropriate.Although the embodiment of the present invention has been described with respect to an example in which the present invention is applied to a cache memory, the present invention is not limited to this. That is, the present invention can be applied to a data processing device having a memory system of a preceding operation type. Here, the preceding operation type is as follows. For example, there are two independent memories each having an address decoder, and when reading data from one of the two memories at a high speed, an access is made in terms of which data is finally adopted. It refers to a memory access operation in which both memories are accessed and data is pre-read and prepared before the address is determined, and after the address is determined, which memory is used to select the data to be read is used. In this case, the memory is  It may be a random access memory (RAM) or a read only memory (ROM).The register for supplying the memory address is not limited to the program counter, and may be a general-purpose register in the CPU. In that case, the CPU sends a program counter (used for instruction fetch) to the control unit having a function such as the LPF module of the present invention for the type of register used for supplying the address.  Or a general-purpose register (may be used as a data address pointer) to output a register type notification signal. Needless to say, the LPF module is controlled based on the register type notification signal.A data processing system to which a data processing device such as a microcomputer is applied is not limited to a digital still camera, but may be a portable communication terminal or a CD-RO.  The present invention can be applied to various systems such as an M drive device.According to the present invention, the access control circuit of the memory to be accessed determines whether the previous memory access address and the current memory access address are continuous or discontinuous. If the memory access address of the memory No. is continuous with the access address of the current memory, the memory accessed last time is accessed according to the determination result.[0135]The effects obtained by typical ones of the inventions disclosed in the present application will be briefly described as follows.That is, the flag means guarantees by the first state that the cache line which becomes a cache hit is the same even when accessing by the next continuous access address, so that the first signal indicates the continuous access address state. When the flag means is in the first state,  A cache hit occurs on the same cache line as the previous access. In this state, even if the operation of the address array is suppressed, the cache line selected (indexed) in the data array becomes a cache line including a normal cache entry for the access address signal. The cache memory means can realize low power consumption by inhibiting the operation of the address array when detecting the state. Since the control is not a simple prediction-based control that simply remembers the index address of the previous access (the first part of the access address signal) and compares this with the current index address, an error in the determination result is obtained. Does not occur, and there is no situation in which the process starts again from the address determination, and the delay of the data read operation can be completely prevented. Therefore, while maintaining the data reading speed equivalent to the format in which the address array and the data array operate in parallel,  Low power consumption can be achieved.The data processing system using the data processing device can realize both low power consumption and high-speed memory access.[Brief description of the drawings]FIG. 1 is a block diagram showing a first example of a cache memory applied to a data processing device of the present invention.FIG. 2 is a block diagram of a microcomputer which is an example of a data processing device according to the present invention.FIG. 3 is a block diagram schematically showing a configuration of an address generation unit of the CPU.FIG. 4 is an explanatory diagram showing an instruction address generation path at the time of NIF access by a CPU.FIG. 5 is an explanatory diagram showing an instruction address generation path at the time of FIF access by a CPU.FIG. 6 is an explanatory diagram showing an address generation path at the time of DA access by the CPU.FIG. 7 is an explanatory diagram showing an example of a bus command.FIG. 8 is a block diagram illustrating an example of a memory block.FIG. 9 is a block diagram illustrating an example of an LPF module.FIG. 10 is a logic circuit diagram showing an example of control logic for setting an LPF flag to a set state (first state).FIG. 11 shows a reset state of the LPF flag (second state).  FIG. 3 is a logic circuit diagram showing an example of control logic for implementing the following.FIG. 12 is a logic circuit diagram showing an example of control logic of a latch and a selector in the LFP module.FIG. 13 is a logic circuit diagram showing an example of a selection logic of a way selection information selector.FIG. 14 is a logic circuit diagram showing an example of control logic of an operation control signal common to each memory block of an address array and an operation control signal individual to each memory block of a data array.FIG. 15 is an explanatory diagram showing a signal flow in a LPF hit state by a bold line;FIG. 16 is an explanatory diagram showing a signal flow in a LPF miss state by a bold line.FIG. 17 is a timing chart showing an example of operation timing when a continuous instruction fetch is performed in a cache memory that does not employ the LPF function.FIG. 18 is a timing chart showing an example of an operation when NIF access that becomes an LPF hit continues.FIG. 19 is a timing chart showing an example of an operation in which a cache miss and a cache fill occur when NIF accesses that result in LPF hits continue.FIG. 20 is a timing chart showing an example of an operation when a FIF access is successively a cache hit;FIG. 21 is a block diagram illustrating a second example of a cache memory configured as a unified cache memory.FIG. 22 is a block diagram illustrating a second example of the LPF module.FIG. 23 is a logic circuit diagram showing a second example of the generation logic of the control signals for the latch control and the selector control of the LFP module.FIG. 24 is a logic circuit diagram showing a second example of the reset control logic of the LPF flag.FIG. 25 is a timing chart showing an example of an operation in the case where the first way instruction code is not evicted from a cache line due to a cache fill caused by a cache miss during DA access in the cache memory according to the second example; .FIG. 26 is a timing chart showing an example of an operation in the case where the first way instruction code is evicted from a cache line due to a cache fill caused by a cache miss during DA access in the cache memory according to the second example.FIG. 27 is a block diagram of a cache memory according to a third example in which power consumption upon LPF hit is further reduced.FIG. 28 is a block diagram showing an example of one memory block and a cache line buffer of a cache memory according to a third example.FIG. 29 is a logic circuit diagram showing an example of a flag set control logic of the cache memory according to the third example.FIG. 30 is a logic circuit diagram showing an example of a generation logic of an activation control signal for a memory block of an address array and an activation control signal for a memory block of a data array in a cache memory according to a third example;FIG. 31 shows N in the cache memory according to the third example;  9 is a timing chart showing an example of an operation in an LPF hit state at the time of continuous IF access.FIG. 32 shows N in the cache memory according to the third example;  9 is a timing chart showing an example of an operation when a cache miss occurs in data access at the time of continuous IF access.FIG. 33 is a block diagram illustrating an example of a DSP having a cache memory using a DSP core as access control means.FIG. 34 is a block diagram schematically illustrating an address generation unit of the DSP core.FIG. 35 is an explanatory diagram showing an example of a bus command output by the DSP core.36 is a logic circuit diagram showing an example of an LPF flag set / reset control logic when the bus command shown in FIG. 35 is considered.FIG. 37 is a system block diagram of a digital still camera to which a microcomputer according to an example of the present invention is applied.[Explanation of symbols]DESCRIPTION OF SYMBOLS 1 Microcomputer 3 DSP 4 Cache memory 20 External memory 101 CPU 109 Address array 110 Data array 111 Hitway selector 115-118 Address array memory block 119-122 Data array memory block 115c-118c Comparator 119b-122b Cache line buffer 115h-118h Way hit / miss signal 301, 301A LPF module 313 LPF flag 305-308 Individual activation control signal for memory block of data array 106 Batch activation control signal for memory block of address array 309 Set signal of LPF flag 310 LPF Flag reset signal 314 Latch holding selection information of way 315 Way selection information selector 317 LPF Gender selection information at the time of hit 315 Information indicating cache fill target way 332, 332A, 332B Cache control circuit 607 Index address signal 608 Address decoder 609 Offset address signal 1108 Index address latch 1502 Batch activation control signal for memory block of data array 2001 DSP core 2005 Instruction cache memory 2006 X data cache memory 2007 Y data cache memory