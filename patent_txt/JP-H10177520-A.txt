PROBLEM TO BE SOLVED: To provide a data processor which is improved in address converting speed. SOLUTION: The address translation buffer of this data processor is divided into an address translation buffer 4 for data and an address translation buffer 3 for instruction so that address translation information for instruction can also be stored in the buffer 4 and, when a translation error occurs in the buffer 3, new address translation information can be fetched from the buffer 4. The address translating speed of the data processor can be improved as compared with such a case where the address translation information is obtained from an external address translation table whenever a translation error occurs..(57) [Problem] To provide a data processor in which an address translation operation is speeded up. SOLUTION: An address translation buffer is divided into a data translation buffer and an instruction translation buffer. An instruction address translation information is also stored in a data address translation buffer (4). When this occurs, new address translation information is taken in from the data address translation buffer (4). When a translation error occurs in the instruction address translation buffer, the address translation operation can be speeded up as compared with the case where address translation information is acquired from an external address translation table every time. .1. A central processing unit, and a part of address conversion information for converting a virtual address output from the central processing unit into a physical address is stored, and a physical address corresponding to the virtual address is associated with the address conversion information. A first address translation buffer to be searched, and address translation information relating to an instruction address in the address translation information held by the first address translation buffer. An address is associatively searched from the address conversion information, and when the result of the associative search is a search error, the first address is determined by the virtual address related to the search error.  A second address translation buffer for causing the address translation buffer to perform an associative search and acquiring address translation information retrieved by the associative search..2. A central processing unit, and part of address conversion information for converting a virtual address handled by the central processing unit into a physical address is stored therein.  A first address translation buffer for associatively searching a physical page number corresponding to a virtual page number output by the central processing unit from the address translation information, and an address translation for an instruction address in the address translation information held by the first address translation buffer A second address translation buffer for storing information and associatively searching a physical page number corresponding to a virtual page number output by the central processing unit at the time of instruction fetch from the address translation information; When the result is a search error, the first address translation buffer is associatively searched by the virtual page number related to the search error, and the address translation information retrieved by the associative search is provided to the second address translation buffer. And control means. Data processor..3. The result of the associative search by the second address translation buffer is a search error, and the result of the associative search of the first address translation buffer by the virtual page number related to the search error is a search error. 3. The data processor according to claim 2, wherein the central processing unit writes and controls the address translation information including the virtual page number related to the search error to the first address translation buffer by exception processing. ..4. A cache entry of data is stored in correspondence with a physical page number, a physical page number associatively searched by said first address translation buffer is supplied, and a cache entry corresponding thereto is associatively searched. 3. The data processor according to claim 2, further comprising a data cache memory..5. A method according to claim 1, wherein a part of said data cache memory is mapped to a predetermined area defined by a virtual address, and an access to said predetermined area is detected to cause said data cache memory to perform a random access operation.  5. The data processor according to claim 4, further comprising a RAM area determination control means..6. A cache entry of an instruction is stored in correspondence with a physical page number, a physical page number associatively searched by said second address translation buffer is supplied, and an associative search is performed for a cache entry corresponding thereto. 3. The data processor according to claim 2, further comprising an instruction cache memory..7. A second R which maps a part of the instruction cache memory to a predetermined area defined by a virtual address and detects an access to the predetermined area to cause the instruction cache memory to perform a random access operation.  7. The data processor according to claim 6, further comprising an AM area determination control means..8. The translation information stored in the first address translation buffer has protection information for defining an access right to a page, and the access right to the page is determined based on the protection information of the translation information related to the associative hit. 5. The data processor according to claim 4, further comprising access protection means for determining..9. A physical page number output by an associative hit by an associative search by the first address translation buffer, and the input physical page number is assigned to a physical page allocated to an I / O register space in a data processor. 9. The data cache memory according to claim 8, further comprising a detecting means for judging whether the data cache memory matches the data, suppressing an associative search operation of the data cache memory by detecting the match, and directly accessing the I / O register. Data processor..10. The translation information stored in the first address translation buffer is cache write mode defining information for defining whether write-through or write-back is employed as a write control mode for the data cache memory. And a cache write control means for controlling a cache write mode for the page based on the cache write mode information included in the conversion information related to the associative hit. 4. The data processor according to 4..11. The apparatus further comprises a first index mode designating circuit, wherein the first index mode designating circuit comprises:  6. The data processor according to claim 4, wherein a specific bit of the virtual address and a bit higher than the specific bit are switched and supplied to the data cache memory..12. The apparatus further comprises a second index mode designating circuit, wherein the second index mode designating circuit comprises:  8. The data processor according to claim 6, wherein a specific bit of the virtual address and a bit higher than the specific bit are switched and supplied to the instruction cache memory..13. A central processing unit; a cache memory;  A selection circuit for selecting a cache entry to be stored in the cache memory..14. The cache memory stores a data cache entry corresponding to a physical page number, is supplied with a physical page number associatively searched by an address translation buffer, and associatively searches a cache entry corresponding thereto. 14. The data processor according to claim 13, wherein the data processor is a data cache memory..15. The cache memory stores a cache entry of an instruction in association with a physical page number, is supplied with a physical page number associatively searched by an address translation buffer, and associatively searches a cache entry corresponding thereto. 14. The data processor according to claim 13, wherein said data processor is an instruction cache memory..16. The data processor according to claim 1, wherein the capacity of the first address translation buffer is larger than the capacity of the second address translation buffer..17. A data processing system including a data processor and an external memory connected to the data processor, the data processor comprising: a central processing unit; and a virtual address handled by the central processing unit, being a physical address. A first part in which a part of address translation information for translation is stored, and a physical page number corresponding to a virtual page number output by the central processing unit is associatively searched from the address translation information.  And a physical page number corresponding to a virtual page number output by the central processing unit at the time of instruction fetch, storing address translation information relating to an instruction address among the address translation information held by the first address translation buffer. From the address conversion information, and when the result of the associative search is a search error, the first address conversion buffer is associatively searched by the virtual page number relating to the search error, and the address conversion searched by the associative search is performed. A second address translation buffer for acquiring information..18. The external memory stores address translation information for translating the virtual address into the physical address, and a recently used part of the address translation information is stored in an operating system of the data processor. 18. The data processing system according to claim 17, wherein the data is stored in the first or second address conversion buffer from the external memory under control..19. When the search result of the data cache memory is a read miss, information for one cache line including data relating to the miss is read from the external memory and stored in the data cache memory. The data processing system according to claim 17, wherein:.20. When the search result of the instruction cache memory is a read miss, information for one cache line including the instruction related to the miss is read from the external memory and stored in the instruction cache memory. The data processing system according to claim 17, wherein:.DETAILED DESCRIPTION OF THE INVENTION[0001]BACKGROUND OF THE INVENTION 1. Field of the Invention The present invention relates to a data processor having an address translation buffer, and a data processing system using such a data processor. It is.[0002]2. Description of the Related Art In a virtual memory system, a virtual memory space which is sufficiently larger than a physical memory is prepared, and a process is mapped to the virtual memory space. Here, the term “process” refers to a program that is being executed under the control of an OS (Operating System). Therefore, the process only needs to consider operations on the virtual memory. For mapping from the virtual memory to the physical memory, an MMU (Memory Management Unit) is used. The MMU is usually managed by an OS (Operating System), and exchanges physical memory so that a virtual memory required by a process can be mapped to the physical memory. The replacement of the physical memory is performed with the secondary storage or the like. The MMU also typically has a memory protection feature to prevent one process from accidentally accessing the physical memory of another process.When performing address conversion from an address of a virtual memory (virtual address) to an address of a physical memory (physical address) using the MMU, the address conversion information is not registered in the MMU, or another Incorrect access to virtual memory. At this time, the MMU raises an exception, changes the mapping of the physical memory, and registers new address translation information.[0004] The function of the above MMU can be realized only by software, but if the conversion is performed by software every time the process accesses the physical memory, the efficiency becomes poor. Therefore, an address translation buffer for address translation is prepared on hardware, and frequently used address translation information is stored in the address translation buffer. That is, the address translation buffer is configured as a cache memory for address translation information. The difference from the normal cache memory is that when the address conversion fails, the replacement of the address conversion information is performed exclusively depending on software.[0005] Various cache memories are widely used to speed up data and instruction access.[0006]SUMMARY OF THE INVENTION The present inventors have studied an address translation buffer and a cache memory from the viewpoint of speeding up memory access. For example, a PowerPC 603 RISC Microprocessor User's Manual  (MOTOROLA, 1994). This processor further includes a data cache memory and an instruction cache memory separately. On page 7-15 of this document  It can be seen that PowerPC handles instruction TLB misses separately from data TLB misses. According to the study of the inventor, even if the address translation buffer is separately provided for the instruction and the data,  It has been found that if address translation fails, necessary address translation information must be obtained from an external memory, since there is no mutual relationship, and there is a limit to speeding up memory access.In the case of a cache miss, one cache entry is newly read from the external memory. At this time, if there is no invalid cache entry, LRU (Least Re  A valid cache entry is evicted from the cache memory according to a logic such as cently used.  The cache entry evicted in this manner may contain data or instructions to be used next.  Therefore, it is desirable that instructions or the like that define processing routines requiring particularly high speed always remain in the cache memory. In such a case, the cache memory may be made available as a random access memory. Conceivable. However, if all the areas of the cache memory are set as such, on the contrary, all functions as the cache memory are killed, which may cause inconvenience depending on the application.An object of the present invention is to provide a data processor that can speed up memory access. More specifically, an object of the present invention is to provide a technique for speeding up memory access from the viewpoint of address translation, and to provide a technique for speeding up memory access from the viewpoint of cache memory.The above and other objects and novel features of the present invention will become apparent from the description of the present specification and the accompanying drawings.[0010]The following is a brief description of an outline of a typical invention among the inventions disclosed in the present application.That is, according to a first aspect of the present invention, an address conversion buffer is divided into a data conversion buffer and an instruction conversion buffer, and the data address conversion buffer also stores instruction address conversion information. When a conversion error occurs, new address conversion information is fetched from a data address conversion buffer.More specifically, the data processor (1) includes a central processing unit (2) and a part of address conversion information for converting a virtual address handled by the central processing unit into a physical address. A first address translation buffer (4) for associatively searching a physical address corresponding to an output virtual address from the address translation information, and storing address translation information relating to an instruction address in the address translation information held by the first address translation buffer. Then, at the time of instruction fetch, the physical address corresponding to the virtual address output by the central processing unit is associatively searched from the address conversion information. Causes the address translation buffer to perform an associative search, and the Comprising a second address translation buffer to obtain the distribution (3).Another data processor from such a viewpoint is a central processing unit, and a part of address conversion information for converting a virtual address handled by the central processing unit into a physical address is stored and output by the central processing unit. A first address translation buffer for associatively searching for a physical page number corresponding to the virtual page number to be translated from the address translation information, and storing address translation information for an instruction address in the address translation information held by the first address translation buffer; A second address translation buffer for associatively searching the address translation information for a physical page number corresponding to the virtual page number output by the central processing unit at the time of instruction fetch, and the result of the associative search by the second address translation buffer is a search error. In some cases, the first address is determined by the virtual page number associated with the search error. The translation buffer is associative search, and a buffer control means (320) to provide address translation information retrieved by the associative search in the second address translation buffer.According to the above means, when a translation error occurs in the instruction address translation buffer, new address translation information is fetched from the data address translation buffer. The address conversion operation can be speeded up as compared with the case where the address conversion information is obtained from. This achieves high-speed memory access. In particular, the speed of the instruction address conversion is increased because the operand fetch is performed in accordance with the decoded result of the fetched instruction. Alternatively, the capacity of the instruction address conversion buffer is made smaller (the number of entries is smaller) than that of the data address conversion buffer.When the result of the associative search by the second address translation buffer is a search error, and the result of the associative search of the first address translation buffer by the virtual page number related to the search error is a search error, The central processing unit reads out the address conversion information including the virtual page number related to the search error from the external memory provided outside the data processor and controls the writing to the first address conversion buffer by the exception processing. When the exception processing is completed, the suspended address translation operation is continued.Another aspect of the present invention is to selectively operate only a partial area of a cache memory as a random access memory. In other words, the cache function is suppressed only for the partial area.More specifically, in the data processor, a cache entry of data is stored in correspondence with a physical page number, and a physical page number associatively searched by the first address translation buffer is supplied and corresponded thereto. A data cache memory for associatively searching for a cache entry is further included. At this time, a part of the data cache memory is mapped to a predetermined area (E1) defined by a virtual address, and the data processor detects an access to the predetermined area and randomly accesses the data cache memory. It further includes a first RAM area determination control means (605) to be operated.In the data processor, a cache entry of an instruction is stored in correspondence with a physical page number, and a physical page number associatively searched by the second address translation buffer is supplied. An instruction cache memory (5) for associatively searching for an entry is further included. At this time, a part of the instruction cache memory is mapped to a predetermined area (E1) defined by a virtual address, and the data processor detects an access to the predetermined area and performs a random access operation on the instruction cache memory. The apparatus further includes a second RAM area determination control unit (505).According to the above means, the predetermined areas of the data cache memory and the instruction cache memory are randomly accessed, and the remaining areas of both cache memories are operated as cache memories in which associative search is performed. Improves data processing speed by satisfying both the requirement that desired instructions and data requiring high-speed access always remain in the cache memory and the requirement that recently used instructions and data remain in the cache memory. To contribute.According to still another aspect of the present invention, a bit position of a virtual address is switched to a bit position higher than that in a normal operation as an index address for selecting a cache line of a cache memory. Thus, the cache memory is divided into large address spaces and assigned to the virtual memory space.More specifically, there is further provided an index mode designating means (630) for selectively using higher-order bits of the virtual address for selecting a cache line of the data cache memory.An index mode designating means (53) for selectively using higher bits of the virtual address for selecting a cache line of the instruction cache memory.  0).According to the above means, since the higher-order bits of the virtual address can be used for the index of the cache memory, the cache memory can be divided for each large address space and allocated to the virtual memory space. Thereby, the cache memory of the direct map can be treated as a set associative cache memory in a pseudo manner, which contributes to an improvement in data processing speed.Still another aspect of the present invention is to improve the usability of a data processor.First, an I / O register area (I / O register space) is mapped from a virtual address space (address space on a virtual memory) to a physical address space (address space on a physical memory). That is, the physical page number output by the associative hit by the associative search by the first address translation buffer is input, and whether the input physical page number matches the page allocated to the I / O register space inside the data processor (606), and further includes a detecting means (606) for suppressing the associative search operation of the data cache memory by detecting a match and for directly accessing the I / O register. At this time,  Access protection means for storing the translation information stored in the first address translation buffer, the protection information defining the access right to the page, and determining the access right to the page based on the protection information of the translation information relating to the associative hit; (405). Thereby, storage protection can be performed for the I / O register space.Second, the translation information stored in the first address translation buffer is a cache write mode for defining whether write-through or write-back is employed as a write control mode for the data cache memory. A cache write control unit (614) that has regulation information (WT) and controls a cache write mode for the page based on the cache write mode information included in the conversion information related to the associative hit. In the write-through mode, writing is performed on both the cache memory and the external memory in the case of a cache hit, and writing is performed only on the external memory in the case of a cache miss. In the write-back mode, in the case of a cache hit, the data is written to the cache entry (cache line) related to the hit, and in the case of a cache miss, one cache entry is read from the external memory (cache fill) and the tag address is changed. The data is updated and data is written to the cache line. The dirty bit of the cache line thus cache-filled is set. When the cache line evicted from the cache memory by the cache fill is dirty,  The cache line will be written back to external memory. Thus, in the write-through mode,  The contents of the cache memory and the contents of the external memory are always the same, but the number of accesses to the external memory increases. In the write-back mode, external memory access is reduced, but there is a period when the contents of the cache memory and the external memory are mismatched, and when multiple cache memories share the external memory, the cache memory and the external memory May not be able to maintain consistency. If the write-through mode and the write-back mode can be selected for each page, the relationship between the consistency of the cache memory and the external memory and the access speed can be optimized according to the system configuration and the contents of the process. .A data processing apparatus to which the above data processor is applied has an external memory connected to the data processor and a secondary storage.[0028]BEST MODE FOR CARRYING OUT THE INVENTION[Structure of Data Processor] FIG. 1 is a block diagram of a data processor according to an example of the present invention. The data processor 1 shown in FIG.  2-bit pipeline RISC (Reduced Instruction)  Set Computer) architecture, and the instruction set of the data processor has a fixed length of 16 bits.The data processor 1 has instruction and data TLBs (address conversion buffers) 3 and 4 separately so that instruction access and data access by the central processing unit (CPU) 2 can be parallelized. , The instruction cache memory 5 and the data cache memory (operand cache memory) 6 are also individualized.Although not particularly limited, the data processor 1 handles a virtual address space defined by a 32-bit virtual address and a physical address space defined by a 29-bit physical address. Address conversion information for converting a virtual address to a physical address includes a virtual page number and a corresponding physical page number. The address conversion table is formed in an external memory of the data processor 1. Of the address translation information in the address translation table,  Some of the recently used ones are stored in the address translation buffers 3 and 4. The control is performed by, for example, the OS of the data processor 1.The data TLB (also referred to as a shared TLB) 4 stores a maximum of 64 data and instruction address translation information.  Store the entry. The shared TLB 4 performs an associative search from the address conversion information for a physical page number corresponding to the virtual page number of the virtual address output to the signal line 111 by the CPU 1 for data fetch, and converts the virtual address into a physical address. Here, the virtual address space is divided into units called pages, and is converted into physical addresses in page units.The instruction TLB (also referred to as an instruction TLB) 3 stores a maximum of four entries of instruction-specific address translation information. In particular, the entry held by the instruction TLB3 is used as a part of the instruction address conversion information held by the shared TLB4. The entry from the shared TLB 4 to the instruction TLB 3 is supplied via a signal line 116. This instruction TLB  Reference numeral 3 associates the physical page number corresponding to the virtual page number of the virtual address output from the CPU 1 to the signal line 110 for the instruction fetch from the address conversion information, and converts the virtual address into a physical address. In the case of a search error, an operation for obtaining the target address conversion information from the shared TLB 4 is instructed via the signal line 115.The data cache memory 6 receives the physical address converted by the shared TLB 4 at the time of data fetch via the signal line 120, and performs an associative search for a cache entry based on the physical address. If the search result is a read hit, data corresponding to the physical address is output to the CPU bus 117 from the cache line related to the hit. If the search result is a read miss, the data for one cache line including the data related to the miss is read from the external memory via the bus controller 7 and cache-filled. Is read out. If the search result is a write miss, the operation differs depending on the write-through mode or write-back mode, which will be described in detail later.The instruction cache memory 5 receives the physical address converted by the instruction TLB3 at the time of instruction fetch via the signal line 125, and performs an associative search for the cache entry based on the physical address. If the search result is a read hit, an instruction corresponding to the physical address is output to the signal line 114 from the cache line related to the hit.  If the search result is a read miss, the data for one cache line including the missed instruction is read from an external memory (not shown) via the bus controller 7 and cache-filled. Is supplied to the CPU 2 via the signal line 114.As will be described in detail later, the instruction cache memory 5 and the data cache memory 6 have their data areas selectively random accessible.The shared TLB is connected to the CPU bus 117.  4 and a data cache memory 6, a peripheral module 8 and a self-test circuit 9 are connected. The peripheral module 8 includes, for example, appropriate circuits such as a timer and a serial interface controller.When the self-test circuit 9 receives a self-test instruction from the CPU 2 via the signal line 112, the self-test circuit 9 writes data to the storage areas of the instruction TLB 3, the shared TLB 4, the instruction cache memory 5, and the data cache memory 6, and The read is performed, and the completion of the test is returned to the CPU 2 via the signal line 113. The test results can be read by the CPU 2 via the CPU bus 117. The self-test circuit 9 sends an access address signal for test and write data to the command TLB3 via a signal line 119.  Etc. The read data from the instruction TLB 3, the shared TLB 4, the instruction cache memory 5 and the data cache memory 6 is not particularly limited, but the dedicated signal line 11  It is supplied to the self-test circuit 9 via 8, 125, 126 and 127.The instruction cache memory 5, data cache memory 6, and external bus controller 7 are connected by a cache address bus 121, a cache data bus 1  22 and a control bus not shown. The external bus controller 7 is necessary to fetch data necessary for cache filling of the instruction cache memory 5 and the data cache memory 6 from the external memory, or to write back data of a cache line of the data cache memory 6 to the external memory. Controls the activation of external bus cycles. This external bus controller 7  Are connected to an external address bus 123, an external data bus, and an external control bus (not shown). An external memory (not shown) is connected to the external address bus 123, the external data bus 124, and the like. The data processor 1 shown in FIG.  It is formed on individual semiconductor substrates.[Common TLB] FIG. 2 shows the common TLB 4  Is shown as an example block diagram. This shared TLB 4 has a memory cell array for storing a maximum of 64 TLB entries.  00 and a data array 401. One TL  The B entry is not particularly limited, but the virtual page number V  It has a PN, a valid bit V, a size bit SZ, a physical page number PPN, a flag FLAG, and a cache write mode bit WT. An area for storing the virtual page number VPN, the valid bit V, and the size bit SZ is configured in the address array 400, and includes a physical page number PPN,  Flag FLAG and cache write mode bit WT  Is stored in the data array 401.Although not particularly limited, the virtual address has a page size of 1 Kbyte, as shown in FIG.  It can be selected from 4K bytes, 64K bytes and 1M bytes. The page size is specified by 2 bits of the size bit SZ.The valid bit V indicates the validity of the TLB entry including the valid bit V, and indicates validity by a logical value "1". The flag FLAG includes protection data and the like. The protected data is 2-bit data representing the access right of the page by a code. For example, “00” can only read in the privileged mode, “01” can read and write in the privileged mode, “10” can only read in the privileged mode and user mode, and “11” can be read in the privileged mode and user mode. And writable. The cache write mode bit WT specifies whether the data cache memory 6 operates in the write-through mode or the write-back mode. As described later, the data cache memory 6 can select a write-through mode or a write-back mode in page units.The address array 400 includes, but is not limited to, a CAM (Content Addressable Memory), and the memory cell itself has a comparison function, as is well known. In the search operation, the address array 40  Memory cells of 0 are selected in parallel to perform a comparison operation.  FIG. 2 conceptually illustrates a circuit element that performs the comparison operation as a comparator. Representatively, four comparators 402A to 402D indicate the elements of the comparison circuit corresponding to the area where the virtual page number of one TLB entry is stored, and 402A indicates bit 10 of the virtual address.  Bits 11 and 402B are bits 12 to  Bits 15 and 402C are assumed to be elements for performing the comparator operation of bits 16 to 19 and 402D of the virtual address, and bits 20 to 31 of the virtual address. When the comparison result in each of the comparison circuits 402A to 402D matches all bits, the comparison result signal is set to the logical value “1”.The objects to be compared by the comparators 402A to 402D are bits corresponding to the virtual page number supplied via the signal line 111. What is indicated by 403 is a mask circuit, and a comparison circuit 40 according to the size bit SZ.  The comparison result of 2A to 402C is masked. That is, when the size bit SZ indicates a 1-Kbyte page, the comparison results of the comparison circuits 402A to 402C are not masked at all, and when the size bit SZ indicates a 4-Kbyte page, the comparison result of the comparison circuit 402A is masked. When the size bit SZ indicates a 64 Kbyte page, the comparison results of the comparison circuits 402A and 402B are masked. When the size bit SZ indicates a 1 Mbyte page, the comparison results of the comparison circuits 402A to 402C are masked. According to this example, the masked comparison result is forcibly set to the logical value “1” and output from the mask circuit 403. Mask circuit 4  03 and the valid bit V of the corresponding TLB entry are supplied to the AND circuit 404, and the output of the AND circuit 404 is used as the hit / miss signal 420 of the TLB entry. Actually, the comparison circuits 402A to 402D,  The mask circuit 403 and the AND circuit 404 are provided for each of the 64 storage areas of the TLB entry.  Therefore, if there is a TLB entry including the virtual page number given to the signal line 111, the TLB entry  The output of the AND circuit 404 corresponding to the entry is set to the logical value “1”.The output of each of the AND circuits 404 is used as a selection signal for the corresponding TLB entry line in the data section 401, and the output of the T corresponding to the selection signal of the logical value "1".  The physical page number PPN of the LB entry line is the signal line 1  20, the flag FLAG is set to the access right determination circuit 405.  Then, the cache write mode bit WT is output to the data cache memory 6.The physical page number PPN given to the signal line 120 is given to the data cache memory 6 together with the offset of the virtual address (the physical address offset) given to the signal line 111.The access right judging circuit 405 outputs a signal (not shown) indicating whether the data processor 1 is currently in the user mode or the privileged mode, and whether the access by the CPU 2 is a read or write instruction. It is determined based on this whether the content of the flag FLAG conforms to the current operation mode of the data processor. If not, a protection violation exception signal 408 is given to the CPU 2.The outputs of the respective AND circuits 404 are supplied to a TLB miss determination circuit 406. This TLB  When the miss determination circuit 406 determines that the outputs of all of the AND circuits 404 have the logical value “0”, it supplies a TLB miss exception signal 407 to the CPU 2. CPU 2 uses the T  Upon receiving the LB miss exception, the OS performs, for example, exception processing for adding a TLB entry relating to the TLB miss from the address translation table of the external memory to the shared TLB 4. When adding a TLB entry, the TLB entry to be added is sent from the CPU bus 117 to the selector 409.  Via the address section 400 and the data section 401. At this time, the selection of the TLB entry line is  The index address given to the signal line 111 from U2 is fetched by the selector 410, and is decoded by the index decoder 411, and is performed. Selector 4  The selection control for the bits 09 and 410 is not particularly limited, but is performed by the TLB miss determination circuit 406.Although the details will be described later, when the instruction TLB3 instructs the common TLB4 to perform the search read processing, the virtual page number for the search hit determination in the search read is sent from the signal line 115 to the selector 412.  Given through. TL in this search read process  The output of the B entry is performed on the signal line 116. The index address for selecting the TLB entry in the self test is supplied from the signal line 119 to the selector 41.  0 is given through. Write data in the self test is supplied from the signal line 119 via the selector 409.[Instruction TLB] FIG. 4 shows the instruction TLB3.  Is shown as an example block diagram. This instruction TLB3 has a memory cell array for storing a maximum of four TLB entries, and this memory cell array is an address array 30.  0 and the data array 301. One TLB  The entry is not particularly limited, but the virtual page number VP  N, a valid bit V, a size bit SZ, a physical page number PPN, and a flag FLAG. The area for storing the virtual page number VPN, the valid bit V, and the size bit SZ is configured in the address array 300, and the area for storing the physical page number PPN and the flag FLAG is configured in the data array 301. The page size of the virtual address, the valid bit V, and the contents of the flag FLAG are the same as those described above.The address array 300 is composed of, but not limited to, a CAM. As is well known, the memory cell itself has a comparing function. In the search operation, the memory cells of the address array 300 are selected in parallel to perform a comparison operation. FIG. 4 conceptually illustrates a circuit element for performing the comparison operation as a comparator. The four comparators 302A to 302D which are typically shown are one TL  This indicates the elements of the comparison circuit corresponding to the area where the virtual page number of the B entry is stored, where 302A is bits 10 to 11 of the virtual address, 302B is bits 12 to 15 of the virtual address, and 302C is virtual Bits 16 to 19 and 302D of the address are assumed to be elements for performing the comparator operation of bits 20 to 31 of the virtual address. When the comparison result in each of the comparison circuits 302A to 302D matches all bits, the comparison result signal is set to the logical value “1”.The objects to be compared by the comparators 302A to 302D are bits corresponding to the virtual page number supplied via the signal line 110. Reference numeral 303 denotes a mask circuit, and a comparison circuit 30 according to the size bit SZ.  The comparison results of 2A to 302C are masked. That is, when the size bit SZ indicates a 1-Kbyte page, the comparison results of the comparison circuits 302A to 302C are not masked at all, and when the size bit SZ indicates a 4-Kbyte page, the comparison result of the comparison circuit 302A is masked. When the size bit SZ indicates a 64 Kbyte page, the comparison results of the comparison circuits 302A and 302B are masked, and when the size bit SZ indicates a 1 Mbyte page, the comparison results of the comparison circuits 302A to 302C are masked. According to this example, the masked comparison result is forcibly set to the logical value “1” and output from the mask circuit 303. Mask circuit 3  03 and the valid bit V of the corresponding TLB entry are supplied to the AND circuit 304, and the output of the AND circuit 304 is used as the hit / miss signal 320 of the TLB entry. Actually, the comparison circuits 302A to 302D,  The mask circuit 303 and the AND circuit 304 are provided for each of the four storage areas of the TLB entry. Therefore, when there is a TLB entry including the virtual page number given to the signal line 110, the output of the AND circuit 304 corresponding to the TLB entry is set to the logical value “1”.The output of each of the AND circuits 304 is used as a selection signal for the corresponding TLB entry line in the data section 301, and the output of the T corresponding to the selection signal of the logical value "1".  The physical page number PPN of the LB entry line is the signal line 1  25, the flag FLAG is set in the access right determination circuit 40.  5 is output.The physical page number PPN given to the signal line 125 is given to the instruction cache memory 5 together with the offset of the virtual address (the physical address offset) given to the signal line 110 and the like.The output of each AND circuit 304 is supplied to a search read control circuit 320 for instructing the common TLB 4 to perform a search read process. The search read control circuit 320 is connected to all the AND circuits 3  04 is determined to be a logical value “0” (instruction TLB miss), the necessary instruction TLB is output from the shared TLB 4.  Start control to read entry. That is,  The virtual page number relating to the instruction TLB miss and the necessary control signal are provided to the shared TLB 4 via the signal line 115. As a result, the shared TLB 4 accesses the address section 400 in parallel and searches for a TLB entry that matches the virtual page number given from the signal line 115. If this search is a hit, the TLB entry (VP  N, V, SZ, PPN, FLAG) are all signal lines 11  6 (in this manner, the information output from the shared TLB 4 to the outside when the search and read processing is instructed from the instruction TLB 3 also includes the contents of the address array 400, This is different from the case of a normal search read in TLB4). Instruction TL  B3 fetches the TLB entry supplied from the shared TLB4 from the selector 309. At this time, the index address is obtained from the search read control circuit 320 by the selector 31.  0 to the index decoder 311.  At the time of adding the TLB entry, although not particularly limited, the search read control circuit 320 can replace the TLB entry by the logic of the LRU.When the search result from the instruction TLB3 to the shared TLB4 by the search read processing instruction is a search miss, the TLB miss determination circuit 406 notifies the CPU 2 of a TLB miss exception. As a result, the CPU 2 adds an entry relating to the TLB miss exception to the shared TLB 4 from an address conversion table of an external memory (not shown) as described above. After completion of this exception handling, the interrupted instruction is re-executed, this time by the shared TLB  As a result of obtaining a search hit in step 4, the TLB entry necessary for the instruction TLB3 is supplied to the instruction TLB3 via the signal line 116 as described above.When a TLB entry is added to the instruction TLB 3, the TLB entry to be added is sent from the signal line 116 to the address unit 300 and the data unit 301 by the selector 309.  It is taken in. The selection of the TLB entry line at this time is performed by fetching the index address given from the search read control circuit 320 by the selector 310 and decoding it by the index decoder 311.  The selection control for the selectors 309 and 310 is not particularly limited, but is performed by the search read control circuit 320.Although details will be described later, an index address for selecting a TLB entry in a self test is provided from a signal line 119 through a selector 310. Write data in the self test is supplied from a signal line 119 via a selector 309. The read operation in the self test is performed for one indexed TLB entry as a whole, and all of the indexed TLB entries are supplied to the signal line 118.[Address Conversion in Instruction Access] Here, the procedure of address conversion processing in instruction access will be described with reference to FIGS. When the instruction access by the CPU 2 is started (start of instruction fetch by the CPU 2), the instruction TLB 3 searches for the presence or absence of a TLB entry corresponding to the instruction address, and determines whether the search hit or miss (S1). If there is, a physical address corresponding to the virtual address is output (S2). If there is a search error in step S1, according to the instruction from the search read control circuit 320, the common TL  B4 searches for the presence or absence of a TLB entry corresponding to the instruction address (S3), and determines whether it is a search hit or a miss (S4). If it is a search hit, the TLB entry corresponding to the virtual page number of the instruction address is searched for. Register in TLB3. After registration, the process returns to S1. Step S  If the determination result of No. 4 is a search error, the TLB miss determination circuit 406 generates a TLB miss exception. T  When the LB miss exception occurs, the CPU 2 interrupts the current process and performs the save process (S10), and then shares the TLB entry of the virtual page number related to the miss with the shared TLB4.  (S11), and finally performs a return process (S1).  2). After the return, the process returns to step S1 in FIG.  As described above, the processing S for the search error of the instruction TLB3  3, S4 and S5 are hardware /  We deal with mishandling.[Data Cache Memory] FIG. 7 shows an example of the data cache memory 6. The data cache memory 6 has a memory cell array for configuring a maximum of 512 cache lines, and this memory cell array includes an address array 600 and a data array 601. One cache line is a cache tag (address tag) composed of physical page numbers  The CTAG includes a valid bit V, a dirty bit U, and a corresponding 32-byte data DATA. The cache tag CTAG, the valid bit V and the dirty bit U are arranged in the address array 600, and the data DATA is arranged in the data section 601. The valid bit V indicates whether valid data is included in the cache line. The logical value “1” indicates valid, and “0” indicates invalid. The dirty bit U is used when the data cache memory 6 is used in the write-back mode, and is set to a logical value “1” when writing occurs in the write-back mode.  By this dirty bit U, it is possible to know the mismatch between the data of the corresponding entry and the data of the external memory. This dirty bit U is initialized to a logical value "0" by a power-on reset.The data cache memory 6 is, although not particularly limited, a direct map. The selection of a cache line is performed by the index decoder 602. The index address is supplied from the control circuit 603 to the selector 60.  4. The control circuit 603 includes the signal line 111  , A RAM area determination control unit 605, an index mode designation unit 630, and an I / O register region detection unit, which will be described in detail later. 606 is provided.The cache tag of the indexed cache line is compared with the corresponding physical page number by the comparator 607. This physical page number corresponds to signal line 12  0 from the shared TLB4. When the cache tag CTAG matches the physical page number and the valid bit V has the logical value “1”, the cache hit signal 608 output from the comparator 607 has the logical value “1” (cache hit). When a cache hit is notified by the cache hit signal 608, the gate 609 passes the data of the indexed cache line to the subsequent stage. Gate 60 by cache hit  A part of the data passed through 9 is selected by the selector 610 and supplied to the bus control circuit 611. The selector 610 performs a selecting operation using a part of the offset address. A part of such an offset address is cut out by the control circuit 603 and supplied through a signal line 623.The bus control circuit 611 is connected to the selector 6  10, the CPU bus 117, the cache data bus 122, the cache address bus 121, etc.  Further, a cache hit signal 608, a physical address from the signal line 616, a read signal and a write signal 615 from the CPU 2, and the like are supplied. This bus control circuit 611  Controls the output of read data relating to a cache hit output from the selector 610 to the CPU bus 117, the control of outputting a physical address for external memory access to the cache address bus 121 in the event of a cache miss, and the control of data from the external memory. Write (cache fill) control via the selector 612, the selector 622 is added to the address portion of the cache line after the cache fill.  Control for writing the cache tag CTAG via the CPU, outputting data to the cache data bus 122 at the time of writing back data to the external memory, and outputting the write-back destination address to the cache address bus 121. The bus control circuit 611 includes a write-back buffer 613 in addition to the logic for the above control. When it becomes necessary to flush a dirty cache entry (U = 1 cache line) to an external memory due to a cache miss, the write-back buffer 613 stores an entry to be flushed in order to give priority to a cache fill operation and improve performance. It is a data buffer for storing, and has a storage area for data of one cache entry and a physical address of an eviction destination.The cache write control circuit 614 controls a write-through mode and a write-back mode for the data cache memory 6. Which operation mode is controlled is determined by the cache write mode bit WT included in the TLB entry.The contents of control by the bus control circuit 611 and the cache write control circuit 614 will be described separately for an associative read operation and an associative write operation.When a data read request is issued from the CPU 2 to a cacheable area, a cache line is selected by the index address indicated by a part of the virtual address, and the cache tag CTAG is read from the selected cache line. . The read cache tag is compared with the physical page number supplied from the shared TLB 4. If the cache tag matches and the valid bit V has the logical value "1", a cache hit is determined, and for example, long word data is output from the selector using a part of the offset of the virtual address. The read data is provided to the CPU bus 117 by the bus control circuit 611. If the tag addresses do not match or the valid bit V is a logical value “0”, a cache miss is determined, and  The bus control circuit 611 reads and controls data of one cache entry from the external memory corresponding to the physical address related to the miss via the selector 612. This data reading operation is called a cache fill. After necessary data is stored in the data array 601 by the cache fill, the valid bit V of the cache line is set to the logical value “1”, and the cache tag CTAG is updated.  The necessary data is returned to the CPU 2. If the cache entry evicted from the cache data array 601 during the cache fill is dirty, the cache is filled after the dirty cache entry is evicted to the write-back buffer 613. Writing back from the write-back buffer 613 to the external memory is performed after the cache fill is completed.When a data write request is issued from the CPU 2 to a cacheable area, a cache hit is determined in the same manner as in the read operation. In the case of a cache hit, if the write-back mode is instructed, data is written to the hit cache entry, and the dirty bit U = 1. In the write-through mode, after writing data to the hit entry, the data is written to the external memory. In this case, no operation is performed on the dirty bit U. When a cache miss occurs, in the write-back mode, the cache is filled, the valid bit V = 1, the dirty bit U = 1, the cache tag is updated, and the data is written to the data cache memory. In write-through mode,  In a cache miss, writing is performed only to the external memory. No cache fill is performed. When a cache miss occurs in the write-back mode, the processing when the entry evicted by the cache fill operation is dirty is the same as the read operation.The data cache memory 6 has a RAM mode and an index mode. The RAM mode is an operation mode in which half of the data array 601 can be randomly accessed as a RAM. In the RAM mode, cache entries 0 to 127 and 256 to 383 function as a cache memory, and cache entry 1  28 to 255 and 384 to 511 are made randomly accessible. The index mode is an operation mode in which a cache memory is divided and a virtual address space is allocated by switching a bit position of a virtual address for selecting a cache line. The RAM mode and the index mode are independently selected by setting a predetermined control bit of the control register 620 to a logical value “1”. When neither the RAM mode nor the index mode is used, the address array 600 and the data array 601 are all used as a cache memory.The RAM area of the data cache memory is  As shown in FIG. 8, which shows the virtual address space of the data processor 1, it is mapped to 0x7C00 0000 to 0x7FFF FFFF. 0x means a hexadecimal number.The RAM area determination control means 605  Switching between a random access operation to the AM area and an operation as a cache memory is controlled. For example, as shown in FIG. 9, the upper 6 bits s2_a [31] to  Inverter IN to detect 0x7C by s2_a [26]  A V1, 6 input AND gate AND1 is provided.  In FIG. 9, s2_a [13] to s2_a [5] are 9 bits included in the virtual address, and are regarded as an index address. Output of AND gate AND1 and address bit  One of s2_a [12] is selected by the selector SEL1. The selection operation of the selector SEL1 is controlled by the control signal 6  21. The control signal 621 has a logical value corresponding to one bit of the control register 620, and the one bit is a control bit for designating the RAM mode.  When the RAM mode is designated, the selector SEL1 selects the output of the AND gate AND1. AND gate AN  The output of D1 is the upper 6 bits s2_a [31] to  Only when s2_a [26] is 0x7C is the logical value “1”,  Otherwise, the logical value is set to "0". Therefore, RA  In the M mode, the upper 6 bits s2_a [31] to  When s2_a [26] is 0x7C, the address array 600 and the data array 601 store the entries 128 to 255 and 38.  Areas 4 to 511 are to be indexed. For other addresses, entries 0 to 127 and 256 to 3  83 areas are indexed. In the RAM mode, when the output signal of the AND gate AND1 has the logical value “1”, the gate 609 and the bus control circuit 611  Masks the cache hit signal 608. And  The selector 610 and the bus control circuit 611 on the data array 601 side provide the remaining part s2_a [23] to s2_a of the virtual address.  Using [14] and s2_a [4] to s2_a [2], random read in 32-bit units from the data array 601 is enabled. When performing a random write to the data array 601,  The write data is sent from the CPU bus 117 to the selector 612.  Is supplied via The control of the selector 612 in the random write is performed in the same manner as in the random read.  11 is the partial virtual addresses s2_a [23] to s2_a [14] and s2  _a [4] to s2_a [2].Since the random access in the RAM mode is a direct access to the RAM area mapped in the virtual space, the access is performed between the CPU bus 117 and the CPU 2. Even when the RAM mode is set, the data cache memory 6 can still perform the above-described cache operation using a half storage area of the data cache memory 6 for a memory access other than the RAM area. R  According to the above description, whether the operation is the AM operation or the cache operation,  It is determined based on the output of the AND gate AND1 in FIG.The index mode designating means 630  Switches the bit position of the virtual address for selecting the cache line, thereby dividing the cache memory and assigning it to the virtual address space. For example, as shown in FIG. 9, the 25th bit s2_a [25] and 13  The bit s2_a [13] is selected by the selector SEL2.  When the RAM mode is not used, the output of the selector SEL2 is used as an index together with the virtual addresses s2_a [12] to s2_a [5]. The selection operation of the selector SEL2 is controlled by the control signal 621. The control signal 621 has a logical value corresponding to one bit of the control register 620,  The one bit is a control bit for designating the index mode. When index mode is specified,  The selector SEL2 selects s2_a [25]. If the index mode is not specified, s2_a [23] is selected. When the index mode is specified, since s2_a [25] is used for the index, the use of the upper side and the lower side of the data cache memory is divided every 32 Mbytes. By arranging the program on the 32 Mbyte boundary, the data cache memory can be handled in a pseudo two-way set associative manner.The I / O register area determining means 606  Indicates that the physical page number supplied from the shared TLB 4 is I /  It is determined whether the page number matches the page number assigned to the O register area. That is, in the data processor 1, as illustrated in FIG. 10, 0x1F00 0000 to 0x1FFF FFFF in the physical address space are allocated to the I / O register area. The I / O register area means an area to which a register included in the peripheral module 8 and a register such as the control register 620 included in the data cache memory 6 are allocated, and a register such as a general-purpose register of the CPU 2 and a data If the processor includes a floating point unit, this means excluding registers such as floating point registers. As described above, the I / O register area is a register area which is accessed by designating the address to which it is mapped. I / O  The register area detecting unit 606 determines whether the upper 5 bits of the physical page number output as a result of the associative hit by the shared TLB 4 are all-bit logical values “1” (0x1F). When an access to the I / O register space is detected, the signal  Given to one. As a result, the bus control circuit 611  In addition to suppressing data input / output due to a cache operation (associative search operation) of the data cache memory 6,  The bus control for directly accessing the I / O register is performed. For this bus control, a physical address given to the bus control circuit 611 via the shared TLB 4 is used.  Also at this time, the access right protection circuit 405 described in FIG.  Monitors the protection information (included in the FLAG) included in the TLB entry, so that the memory protection can be performed even in the I / O register space. In this way, by mapping the I / O register area (I / O register space) from the virtual address space (address space on the virtual memory) to the physical address space (address space on the physical memory), the I / O register Memory protection can also be performed on space.In the self-test mode, write data and an address signal are supplied to the data cache memory 6 via a signal line 119, and the address signal is supplied to an index decoder 602 via a selector 604. The write data is supplied to the data array 601 and the address array 600 via the selectors 612 and 622. Read data from the address array 600 and the data array 601 are supplied to the self-test circuit 9 via a dedicated signal line 127.[Instruction Cache Memory] FIG. 11 shows an example of the instruction cache memory 7. The instruction cache memory 5 has almost the same basic configuration as the data cache memory 6 except that the instruction cache memory 5 does not have a write-back / write-through switching function and a direct access function to an I / O register area. is there. Here, the description will focus on differences from the data cache memory 6.The instruction cache memory 5 has a memory cell array for configuring a maximum of 256 cache lines, and this memory cell array includes an address array 500 and a data array 501. One cache line includes a cache tag (address tag) CTAG constituted by a physical page number, a valid bit V, and 16 instructions IST corresponding thereto. The cache tag CTAG and the valid bit V are stored in the address array 500.  The instruction IST is arranged in the data array 501.The instruction cache memory 5 is not particularly limited, but is a direct map. The selection of a cache line is performed by the index decoder 502. The index address is supplied from the control circuit 503 via the selector 504. The control circuit 503 performs control for distributing the virtual address supplied from the signal line 110 and the physical page number supplied from the signal line 125 to each unit.  A RAM area determination control unit 505 and an index mode designation unit 530 are provided.The cache tag of the indexed cache line is compared with the corresponding physical page number in the comparator 507. This physical page number is supplied from the instruction TLB3 via the signal line 125. The cache tag matches the physical page number, and the valid bit V is a logical value “1”  (Cache hit), the cache hit signal 508 output from the comparator 507 is set to the logical value “1”. When a cache hit is notified by the cache hit signal 508, the gate 509 passes the data of the indexed cache line to the subsequent stage. A part of the data that has passed through the gate 509 due to the cache hit is selected by the selector 510 and supplied to the bus control circuit 511. Selector 510  Performs a selection operation using a part of the offset address. Some of such offset addresses are stored in the control circuit 5  03, and are supplied via a signal line 523.The bus control circuit 511 is connected to the selector 5  10 output, CPU bus 117, cache data bus 122, cache address bus 121, signal line 114  And a cache hit signal 508, a physical address from the signal line 516, a read signal and a write signal 515 from the CPU 2, and the like. The bus control circuit 511 controls to output read data relating to a cache hit output from the selector 510 to the signal line 114, controls to output a physical address for external memory access to the cache address bus 121 when a cache miss occurs, Selector 512 selects data from memory  (Cache fill) control via the selector 522 and control via the selector 522 to write the cache tag CTAG into the address portion of the cache line after the cache fill.The contents of control by the bus control circuit 511 will be described. When an instruction read request is issued from the CPU 2 to a cacheable area, a cache line is selected by an index address indicated by a part of the virtual address, and a cache tag is read from the selected cache line. The read cache tag is compared with the physical page number supplied from the instruction TLB3. When the cache tag matches and the valid bit V is a logical value “1”, a cache hit is determined, and for example, long word data is output from the selector 510 using a part of the offset of the virtual address. The read data is supplied to the CPU 2 by the bus control circuit 511 via the signal line 114. If the tag address does not match or the valid bit V has the logical value “0”, a cache miss is determined, and the bus control circuit 511 transfers the data for one cache entry from the external memory corresponding to the physical address related to the miss to the selector 512. Read via. This data reading operation is called a cache fill. After the necessary data is stored in the data array 501 by the cache fill, the valid bit V of the cache line is set to the logical value “1”, and the cache tag CTAG is updated via the selector 522. returned. Since there is no instruction write when the CPU 2 fetches an instruction, even if an old cache entry is evicted from the instruction cache memory 5 at the time of cache filling, there is no need to write back to the external memory.The instruction cache memory 5 also has the same RA  It has an M mode and an index mode. When the RAM mode is set, half of the data array 501 is RAM  As random access. Cache entries 0-63 and 128-191 in RAM mode  Function as a cache memory, and cache entries 64-127 and 192-255 are made randomly accessible. The RAM mode and the index mode are independently selected by setting a predetermined control bit of the control register 520 to a logical value “1”. Otherwise, the address array and data array 501 are all used as cache memory.The RAM area of the instruction cache memory 5  As shown in FIG. 8 showing the virtual address space of the data processor 1, the address is mapped to 0x7C00 0000 to 0x7FFF FFFF.The RAM area determination control means 505  Switching between a random access operation to the AM area and an operation as a cache memory is controlled. The logic is shown in FIG.  However, since the number of cache lines is half that of the data cache memory, the index position is shifted down by one bit. The setting of the operation mode is determined by one bit of the control register 520, and the value of the bit is determined by the control signal 521 in the RAM area determination control unit 50.  5 given. When the RAM mode is designated, if the upper 16 bits of the virtual address are 0x7C, the address array 500 and the data array 501 have the entries 64 to  127 and 192 to 255 are indexed. For other addresses, the areas of entries 0 to 63 and 128 to 191 are to be indexed. In RAM mode, the access address is RAM  If it is a region, the gate 509 and the bus control circuit 511  Masks the cache hit signal 508 and selects the selector 5  10 and the bus control circuit 511 enable random reading from the data array 501 in 32-bit units.Since the random access in the RAM mode is a direct access to the RAM area mapped in the virtual space, the access is performed on the signal line 1.  14 and the CPU 2. Even when the RAM mode is set, the instruction cache memory 5 can still perform the above-described cache operation by using a half storage area of the instruction cache memory 5 for a memory access other than the RAM area.The index mode specifying means 530  Switches the bit position of the virtual address for selecting the cache line, thereby dividing the cache memory and assigning it to the virtual address space. The logic can be configured in the same manner as in FIG. 9, but the number of cache lines is half that of the data cache memory, so the position of the index is shifted down by one bit.The index mode selecting operation is performed by the control signal 5  21. The control signal 521 has a logical value corresponding to one bit of the control register 520, and the one bit is a control bit for designating the index mode. When the index mode is specified, the 25th bit of the virtual address is used for the index,  The use of the above side and the lower side of the instruction cache memory is separated every 2 Mbytes. By arranging the program on the 32 Mbyte boundary, the instruction cache memory can be handled in a pseudo two-way set associative manner.In the self-test mode, write data and an address signal are supplied to the instruction cache memory 5 through a signal line 119, and the address signal is  04 to the index decoder 502. The write data is supplied to the data array 501 and the address array 500 via the selectors 512 and 522. Read data from the address array 500 and the data array 501 is supplied to the self-test circuit 9 via a dedicated signal line 126.[Self Test Circuit] FIG. 12 is a block diagram of the self test circuit 9. The self test circuit 9 uses the test setting circuit 900 to store test data in the instruction TLB3, the shared TLB4, and the instruction cache memory 5.  Then, the data is written to the data cache memory 6 and the written data is supplied to the instruction TLB determination circuit 903, the shared TLB determination circuit 904, the instruction cache determination circuit 905, and the data cache determination circuit 906, respectively. Each of the determination circuits 903 to 906 determines, for example, a match between the corresponding write data and read data. The determination result is held in the result register 907, and is stored in the C through the bus control circuit 908.  It is made readable by PU2.The activation of the self-test is determined by the activation determination circuit 909 based on the signal 112 from the CPU 2. When the start of the self test is instructed, the start determination circuit 909  Starts the state machine 910 and sequentially repeats the control cycle for the test operation. The test control circuit 900 synchronizes with the control cycle to execute the instruction TLB3, the shared TL  B4, start a write cycle and a read cycle to the instruction cache memory 5 and the data cache memory 6,  Instruction TLB determination circuit 903, shared TLB determination circuit 90  4. The determination operation of the instruction cache determination circuit 905 and the data cache determination circuit 906 is controlled. After the result of one determination is read by the CPU 2, the result register 907 is updated to the initial value by the update circuit 911, and these operations are repeated to the end. Completion of the self test is based on the output of the state machine 910,  12 makes a decision, and returns the result to the CPU 2 as a signal 113. The setting of test conditions such as write data and write address for the test setting circuit 900 is performed by the register setting circuit 913 controlled via the signal 112.[Data Processing System] FIG. 13 shows an example of a data processing system to which the data processor 1 is applied. In the figure, 1 is the data processor,  11 is a dynamic random access memory (D  RAM) and 12 are DRAM control units for performing address multiplex control and refresh control on the DRAM 11, and 13 is an SRAM. The SRAM 13 is used as a work area of the data processor 1 and a temporary storage area for data. Reference numeral 14 denotes an OS (Operating Operating System) of the data processor 1.  System). Reference numeral 15 denotes a peripheral device control unit, to which an external storage device 16 and a keyboard 17 typically shown are connected. Reference numeral 18 denotes a display controller provided with a frame buffer 19 and a drawing and display control logic circuit (not shown), which performs drawing control and display control on the display 20. 21 is a power supply circuit, and 22 is a bus typically shown.The DRAM 11, SRAM 13 and RO  M14 and the like constitute an external memory of the data processor 1, and the external storage device 16 is used as secondary storage of the external memory. The address conversion table is formed in, for example, an SRAM or a DRAM.[Advantage of Data Processor] According to the data processor 1 described above, the address translation buffers (TLBs) 3 and 4 are divided into those for data and those for instructions, and the address translation buffers 4 for data are stored in the address translation buffers 4 for data. Address translation information is also stored, and when a translation error occurs in the instruction address translation buffer 3, new address translation information is fetched from the data address translation buffer 4. Therefore, when a translation error occurs in the instruction address translation buffer 3, new address translation information is fetched from the data address translation buffer 4, and every time a translation error occurs, the address translation information is read from the external address translation table. , The address conversion operation can be speeded up as compared with the case of acquiring This makes it possible to achieve high-speed memory access.Only a partial area of the cache memories 5 and 6 can be selectively operated as a random access memory. According to this, the RAM areas of the data cache memory 6 and the instruction cache memory 5 are randomly accessed, and the remaining areas of both the cache memories 5 and 6 are operated as cache memories for performing an associative search. It is possible to satisfy both the requirement that desired instructions and data requiring high speed always remain in the cache memories 5 and 6 and the requirement that recently used instructions and data remain in the cache memories 5 and 6.  This can contribute to an improvement in data processing speed.An index address for selecting a line of the cache memories 5 and 6 can be switched. As a result, higher-order bits of the virtual address can be selectively used for selecting a line of the cache memory, so that the cache memory of the direct map can be treated as a pseudo-set associative cache memory, and data processing can be performed. This can contribute to an increase in speed.The I / O register area is mapped from the virtual address space to the physical address space. At this time, TL  The B entry has protection information defining the access right to the page, and the access right determination circuit 405 determines the access right to the page based on the protection information of the conversion information related to the associative hit. Therefore, storage protection can be performed on the I / O register space.The entry of the shared TLB 4 has a cache write mode bit WT for specifying whether write-through or write-back is to be used for the data cache memory 6. And the cache write mode bit W  The cache write control mode is determined with reference to T.  In the write-through mode, the contents of the cache memory and the contents of the external memory always match, but the number of accesses to the external memory increases. In the write-back mode, external memory access is reduced, but there is a period when the contents of the cache memory and the external memory are mismatched, and when multiple cache memories share the external memory, the cache memory and the external memory May not be able to maintain consistency. Since the write-through mode or the write-back mode can be selected for each page, the relationship between the consistency of the cache memory and the external memory and the access speed can be optimized according to the system configuration and the contents of the process. .Therefore, a data processing system to which the data processor 1 is applied can improve data processing efficiency. Further, the present invention can be applied to various systems having different requests in terms of the use form of the cache memory and the like.Although the invention made by the present inventor has been specifically described based on the embodiment, it is needless to say that the present invention is not limited to the embodiment and can be variously modified without departing from the gist thereof. No.For example, the instruction TLB and the shared TLB can be configured in a direct map or set associative form. The data cache memory and the instruction cache memory can also adopt the set associative format. Also, the data path for the self-test connected to the instruction TLB or the like does not need to be dedicated, and can be shared by gate control or the like. The data processor can also include another circuit module, such as a floating point unit.[0099]The effects obtained by typical ones of the inventions disclosed in the present application will be briefly described as follows.That is, when a translation error occurs in the instruction address translation buffer, new address translation information is fetched from the data address translation buffer.  The address translation operation can be speeded up as compared with the case where address translation information is acquired from an external address translation table every time a translation error occurs. This makes it possible to achieve high-speed memory access.The data cache memory and the instruction cache memory allow a random access to a part of the storage area and enable the cache memory operation by the associative search in the remaining storage area. Therefore, particularly high speed access is required. It is possible to satisfy both the requirement that desired instructions and data always remain in the cache memory and the requirement that recently used instructions and data remain in the cache memory, thereby contributing to an improvement in data processing speed.Since the data cache memory and the instruction cache memory can switch the designated bits of the index address, the cache memory can be divided for each large address space and used, contributing to an improvement in data processing speed. Can be.By mapping the I / O register area from the virtual address space to the physical address space,  Memory protection can be performed on the O register space.Since the write-through mode and the write-back mode can be selected for each page, the relationship between the consistency of the cache memory and the external memory and the access speed can be optimized according to the system configuration and the contents of the process. Will be possible.[Brief description of the drawings]FIG. 1 is a block diagram of a data processor according to an example of the present invention.FIG. 2 is a block diagram illustrating an example of a shared TLB.FIG. 3 is an explanatory diagram of a page size.FIG. 4 is a block diagram illustrating an example of an instruction TLB.FIG. 5 is a flowchart illustrating a procedure of an address conversion process in an instruction access.FIG. 6 is a flowchart illustrating an outline of exception processing for a TLB miss.FIG. 7 is a block diagram illustrating an example of a data cache memory.FIG. 8 is an address map of a virtual address space.FIG. 9 is a block diagram illustrating an example of a RAM area determination control unit and an index mode designation unit.FIG. 10 is an address map of a physical address space.FIG. 11 is a block diagram illustrating an example of an instruction cache memory.FIG. 12 is a block diagram illustrating an example of a self-test circuit.FIG. 13 is a block diagram showing an example of a data processing system to which the data processor of FIG. 1 is applied.[Explanation of symbols] Reference Signs List 1 data processor 2 CPU 3 instruction TLB 11 DRAM 13 SRAM 14 ROM 16 external storage device 300 address array 301 data array 302A to 302D comparator 305 access right determination circuit 320 search read control circuit 4 shared TLB 400 address array 401 data array 402A to 402A 402D Comparator 406 TLB miss determination circuit 405 Access right determination circuit 5 Instruction cache memory 500 Data array 501 Address array 505 RAM area determination control means 511 Bus control circuit 6 Data cache memory 600 Data array 601 Address array 605 RAM area determination control means 606 I / O register area detecting means 611 Bus control circuit ──────────────────────────────────────────────────続 き Continuing on the front page (72) Inventor Susumu Narita 5-2-1, Josuihonmachi, Kodaira-shi, Tokyo Inside Semiconductor Division, Hitachi, Ltd. No. 20-1, Hitachi Semiconductor Co., Ltd. Semiconductor Division (72) Inventor Makoto Toda 1-280 Higashi Koikekubo, Kokubunji-shi, Tokyo Tokyo, Japan Inside Central Research Laboratory, Hitachi, Ltd. 280 Hitachi Central Research Laboratory, Ltd.